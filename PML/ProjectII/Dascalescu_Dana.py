# -*- coding: utf-8 -*-
"""Project2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k3-4uGfo0-COgBSQCXxKgRJbT_Nu-Isv
"""

from google.colab import drive
drive.mount('/content/drive/')

!pip install -U pandas-profiling

!pip install yellowbrick

import os
import re
import sys
import csv
import string
import operator
import functools

import numpy as np
import pandas as pd

import seaborn as sns
from matplotlib import colors
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

from sklearn.svm import SVC
from sklearn.dummy import DummyClassifier

from yellowbrick.cluster import KElbowVisualizer

from sklearn.cluster import KMeans, DBSCAN
from scipy.optimize import linear_sum_assignment

random_state = None
np.random.seed(random_state)

# Load dataset
raw_data = pd.read_csv('/content/drive/MyDrive/PML/data.csv.zip(Unzipped_Files)/data.csv', error_bad_lines=False, warn_bad_lines=False, sep=r'\t')

# Summary of the dataset
print('Number of samples in the dataset: ', len(raw_data))
raw_data.info()

raw_data.describe()

# Check the proportion of null variables
df_null = round(100. * (raw_data.isnull().sum()) / len(raw_data), 2)
for feature, proportion in df_null.items():
  if proportion > 0.0:
    print('Proportion of missing values in feature {0} is {1}'.format(feature, proportion))

raw_data.drop('major', inplace=True, axis=1)
raw_data["country"] = raw_data["country"].fillna("Unknown")

# perform validity check of the data provided by the survey participants
raw_data = raw_data.drop(raw_data[raw_data['VCL6'] == 1].index)
raw_data = raw_data[raw_data['VCL9'] !=1 ]
raw_data = raw_data[raw_data['VCL12'] != 1]

print('Number of samples: ', len(raw_data))

# Drop some of the redundant features 
to_drop = ['VCL{0}'.format(i) for i in range(1, 17)]
raw_data = raw_data.drop(to_drop, axis=1)

to_drop = ['screensize', 'uniquenetworklocation', 'surveyelapse', 'testelapse', 'introelapse', 'source']
raw_data = raw_data.drop(to_drop, axis=1)

to_drop = ['Q{0}I'.format(i) for i in range(1, 43)]
raw_data = raw_data.drop(to_drop, axis=1)

percentiles_list = [.25, .50, .75, .95, .98, .99, .995, .999]
raw_data.describe(percentiles=percentiles_list)

"""
### Statistical summary of the demographic data"""

percentiles_list = [.25, .50, .75, .95, .9975]
 raw_data[['age', 'gender', 'education', 'religion', 'orientation', 'race', 'married', 'familysize']].describe(percentiles=percentiles_list)

raw_data = raw_data[raw_data['familysize'] < np.percentile(raw_data['familysize'], 99.9)]

plt.figure(figsize=(7,3))
plt.boxplot(raw_data['familysize'], vert=False)
plt.title('Number of children in the family (including himself/herself)\n')
plt.show()

items_dass_scales = {
    'Stress': [1, 6, 8, 11, 12, 14, 18, 22, 27, 29, 32, 33, 35, 39],
    'Anxiety': [2, 4, 7, 9, 15, 19, 20, 23, 25, 28, 30, 36, 40, 41],
    'Depression': [3, 5, 10, 13, 16, 17, 21, 24, 26, 31, 34, 37, 38, 42]
}

scores_bins_scales_dass = {
    'Stress': [(0, 14), (15, 18), (19, 25), (26, 33)],
    'Anxiety': [(0, 7), (8, 9), (10, 14), (15, 19)],
    'Depression': [(0, 9), (10, 13), (14, 20), (21, 27)]
}

category_dass = {
    0: 'normal',
    1: 'mild',
    2: 'moderate',
    3: 'severe',
    4: 'extremely severe'
}

for key, item in items_dass_scales.items():
  raw_data[key] = (raw_data.filter(regex='Q(%s)A' % '|'.join(map(str, item))) - 1).sum(axis=1)
  
  bins = scores_bins_scales_dass[key] 
  bins.append((scores_bins_scales_dass[key][-1][1], raw_data[key].max() + 1))
  bins = pd.IntervalIndex.from_tuples(bins, closed='left')
  raw_data[key + '_category'] = np.arange(len(bins))[pd.cut(raw_data[key], bins=bins).cat.codes]

dass_scores = raw_data[items_dass_scales.keys()]
dass_categories = raw_data[[key + '_category' for key in items_dass_scales.keys()]]

"""### Distribution of Stress Severity"""

bins = [0, 14, 18, 25, 33, 56]
sns.displot(dass_scores, 
            bins=bins, 
            kde=True, kind='hist',
            height=5, aspect=1.4, 
            palette='GnBu', x='Stress', hue='Stress')

"""### Distribution of Anxiety Severity"""

bins = [0, 7, 9, 14, 19, 56]
sns.displot(dass_scores, 
            bins=bins, 
            kde=True, kind='hist',
            height=5, aspect=1.4, 
            palette='GnBu', x='Anxiety', hue='Anxiety')

"""### Distribution of Depression Severity"""

bins = [0, 9, 10, 13, 27, 56]
sns.displot(dass_scores, 
            bins=bins, 
            kde=True, kind='hist',
            height=5, aspect=1.4, 
            palette='GnBu', x='Depression', hue='Depression')

personality_types = ['Extraversion', 'Agreeableness', 'Conscientiousness', 'Emotional Stability', 'Opennes']
reg = 'TIPI(%s)' % '|'.join([str(i) for i in range(2, 12, 2)])

tipi_answers = raw_data.filter(regex='TIPI\d+').copy()
tipi_invers = tipi_answers.filter(regex=reg).apply(lambda x: 7-x)
tipi_answers[tipi_answers.columns.intersection(tipi_invers.columns)] = tipi_invers
tipi_answers

# Assigne scores for each personality attribute
for idx, attribute in enumerate(personality_types):
  raw_data[attribute] = tipi_answers[['TIPI{0}'.format(idx + 1), \
                                      'TIPI{0}'.format(7 +idx - 1)]].mean(axis=1)
personalities_types = raw_data[personality_types]
personalities_types

raw_data

avg_extraversion = raw_data[['Depression_category', 'Extraversion']].groupby('Depression_category').mean()
avg_agreeableness = raw_data[['Depression_category', 'Agreeableness']].groupby('Depression_category').mean()
avg_conscientiousness = raw_data[['Depression_category', 'Conscientiousness']].groupby('Depression_category').mean()
avg_emotional_stability = raw_data[['Depression_category', 'Emotional Stability']].groupby('Depression_category').mean()
avg_opennes = raw_data[['Depression_category', 'Opennes']].groupby('Depression_category').mean()

avg_extraversion

plt.figure(figsize = (8, 4))
plt.plot(avg_extraversion, marker='o')
plt.plot(avg_agreeableness, marker='o')
plt.plot(avg_conscientiousness, marker='o')
plt.plot(avg_emotional_stability, marker='o')
plt.plot(avg_opennes, marker='o')
plt.xlabel("Depression Severity")
plt.ylabel("Personality Trait")
plt.title("Depression Severit vs Personality Trait")
plt.legend(personality_types)
plt.show()

avg_extraversion = raw_data[['Anxiety_category', 'Extraversion']].groupby('Anxiety_category').mean()
avg_agreeableness = raw_data[['Anxiety_category', 'Agreeableness']].groupby('Anxiety_category').mean()
avg_conscientiousness = raw_data[['Anxiety_category', 'Conscientiousness']].groupby('Anxiety_category').mean()
avg_emotional_stability = raw_data[['Anxiety_category', 'Emotional Stability']].groupby('Anxiety_category').mean()
avg_opennes = raw_data[['Anxiety_category', 'Opennes']].groupby('Anxiety_category').mean()

plt.figure(figsize = (8, 4))
plt.plot(avg_extraversion, marker='o')
plt.plot(avg_agreeableness, marker='o')
plt.plot(avg_conscientiousness, marker='o')
plt.plot(avg_emotional_stability, marker='o')
plt.plot(avg_opennes, marker='o')
plt.xlabel("Anxiety Severity")
plt.ylabel("Personality Trait")
plt.title("Anxiety Severit vs Personality Trait")
plt.legend(personality_types)
plt.show()

avg_extraversion = raw_data[['Stress_category', 'Extraversion']].groupby('Stress_category').mean()
avg_agreeableness = raw_data[['Stress_category', 'Agreeableness']].groupby('Stress_category').mean()
avg_conscientiousness = raw_data[['Stress_category', 'Conscientiousness']].groupby('Stress_category').mean()
avg_emotional_stability = raw_data[['Stress_category', 'Emotional Stability']].groupby('Stress_category').mean()
avg_opennes = raw_data[['Stress_category', 'Opennes']].groupby('Stress_category').mean()

plt.figure(figsize = (8, 4))
plt.plot(avg_extraversion, marker='o')
plt.plot(avg_agreeableness, marker='o')
plt.plot(avg_conscientiousness, marker='o')
plt.plot(avg_emotional_stability, marker='o')
plt.plot(avg_opennes, marker='o')
plt.xlabel("Stress Severity")
plt.ylabel("Personality Trait")
plt.title("Stress Severit vs Personality Trait")
plt.legend(personality_types)
plt.show()

"""Plot correlation matrix for numerical features."""

features_corr = raw_data.loc[:, ~raw_data.columns.isin(['TIPI%s' % str(i) for i in range(1, 11)] + \
                                                       ['Q%sA' % str(i) for i in range(1, 43)] + \
                                                       ['Q%sE' % str(i) for i in range(1, 43)])].corr()
plt.figure(figsize=(19, 6))
sns.heatmap(features_corr, annot=True, center=0)

labels = {1: 'Never',
          2: 'Sometimes',
          3: 'Almost Always',
          4: 'Always'}

for i in range(1,43):
  raw_data['Q%sA' % str(i)].replace(labels, inplace=True)

# Get list of categorical variables
indexes = (raw_data.dtypes == 'object')
indexes

# Get the categorical features
categorical_features_columns = list(indexes[indexes].index)
print('Categorical variables in the dataset: ', categorical_features_columns)

# Encoding categorical features
label_encoder = LabelEncoder()
for i in categorical_features_columns:
  raw_data[i] = raw_data[[i]].apply(label_encoder.fit_transform)

test_columns = ['Stress', 'Stress_category', 'Anxiety', 'Anxiety_category', 'Depression', 'Depression_category']

X_pd = raw_data.loc[:, ~raw_data.columns.isin(test_columns)]
y_pd = raw_data.loc[:, raw_data.columns.isin(test_columns)]

# Scaling data
scaler = StandardScaler()
scaler.fit(X_pd)
scaled_data_df = pd.DataFrame(scaler.transform(X_pd), columns=X_pd.columns)
scaled_data_df

# X_train, X_test, y_train, y_test = train_test_split(X_pd[['Q%sA' % str(i) for i in range(1, 43)]], \
#                                                     y_pd, test_size=0.25)
X_train, X_test, y_train, y_test = train_test_split(X_pd, \
                                                    y_pd, test_size=0.25)

pca = PCA(n_components = 50)
tsne = TSNE()
# X_embedded = pca.fit_transform(X_train)
X_embedded_tsne = tsne.fit_transform(X_train)
sns.scatterplot(X_embedded_tsne[:, 0], X_embedded_tsne[:, 1], legend='full')

"""## Cluster patients with depression based on the severity of their core symptoms"""

y_train = y_train['Depression_category']
y_test = y_test['Depression_category']

"""#### Random chance"""

dummy_classifier = DummyClassifier(strategy='uniform')
dummy_classifier.fit(X_train, y_train)
print('Accuracy obtained for training data: ', np.mean(dummy_classifier.predict(X_train) == y_train))
print('Accuracy obtained for test data:', np.mean(dummy_classifier.predict(X_test) == y_test))

"""#### Supervised baseline"""

svc = SVC()
svc.fit(X_train, y_train)
print('Accuracy obtained for training data: ', np.mean(svc.predict(X_train) == y_train))
print('Accuracy obtained for test data:', np.mean(svc.predict(X_test) == y_test))

"""Elbow method to determine the number of clusters."""

Elbow_M = KElbowVisualizer(KMeans(), k = 7)
Elbow_M.fit(X_train)
Elbow_M.show()

"""# KMeans"""

kmeans = KMeans(n_clusters=5, random_state=random_state)
kmeans.fit(X_train)
predicted_labels = kmeans.predict(X_train)

def translate_l(labels, col_ind):
    translate_labels = np.zeros(shape=labels.shape)
    for i in range(len(translate_labels)):
        translate_labels[i] = col_ind[labels[i]]
    return translate_labels

n_clusters = 5
confusion_matrix = np.zeros((n_clusters, n_clusters))

for idx, i in enumerate(y_train):
    for idx2, j in enumerate(predicted_labels):
        if idx == idx2:
            confusion_matrix[i][j] += 1

print('Confusion matrix:\n', confusion_matrix)
# for i in range(n_clusters):
#     confusion_matrix[:, i] = confusion_matrix[:, i] / np.sum(confusion_matrix[:, i])
print(confusion_matrix)

row_ind, col_ind = linear_sum_assignment(-1. * confusion_matrix)
print(row_ind, col_ind)
new_labels_train = translate_l(predicted_labels, col_ind)

print(np.mean(new_labels_train == y_train))

predicted_labels_test = kmeans.predict(X_test)
translated_labels_test = translate_l(predicted_labels_test, col_ind)
print(np.mean(translated_labels_test == y_test))

"""We will use normalization of the data."""

X_train, X_test, y_train, y_test = train_test_split(scaled_data_df, y_pd, test_size=0.25)
y_train = y_train['Depression_category']
y_test = y_test['Depression_category']

kmeans = KMeans(n_clusters=5, random_state=random_state)
kmeans.fit(X_train)
predicted_labels = kmeans.predict(X_train) 
predicted_labels

n_clusters = 5
confusion_matrix = np.zeros((n_clusters, n_clusters))

for idx, i in enumerate(y_train):
    for idx2, j in enumerate(predicted_labels):
        if idx == idx2:
          confusion_matrix[i][j] += 1

print('Confusion matrix:\n', confusion_matrix)

row_ind, col_ind = linear_sum_assignment(-1. * confusion_matrix)
print(row_ind, col_ind)
new_labels_train = translate_l(predicted_labels, col_ind)

print(np.mean(new_labels_train == y_train))

predicted_labels_test = kmeans.predict(X_test)
translated_labels_test = translate_l(predicted_labels_test, col_ind)
print(np.mean(translated_labels_test == y_test))

"""Reduce the dimension of the data to 3 components with the PCA algorithm and use the output as a parameter."""

pca = PCA(n_components = 3)
X_embedded = pca.fit_transform(X_train)
X_embedded_test = pca.transform(X_test)

kmeans = KMeans(n_clusters=5, random_state=random_state)
kmeans.fit(X_embedded)
predicted_labels = kmeans.predict(X_embedded) 
predicted_labels

n_clusters = 5
confusion_matrix = np.zeros((n_clusters, n_clusters))

for idx, i in enumerate(y_train):
    for idx2, j in enumerate(predicted_labels):
        if idx == idx2:
          confusion_matrix[i][j] += 1

print('Confusion matrix:\n', confusion_matrix)

row_ind, col_ind = linear_sum_assignment(-1. * confusion_matrix)
print(row_ind, col_ind)
new_labels_train = translate_l(predicted_labels, col_ind)
print(np.mean(new_labels_train == y_train))

predicted_labels_test = kmeans.predict(X_embedded_test)
translated_labels_test = translate_l(predicted_labels_test, col_ind)
print(np.mean(translated_labels_test == y_test))

"""# DBSCAN

"""

n_samples_train, n_samples_test = len(X_embedded), len(X_embedded_test)

distances_points = np.zeros((n_samples_train, n_samples_train))

"""Investigate DBSCAN model with Euclidian distance. We will evaluate the distribution of the samples in the training data and use them as a reference point when deciding the hyperparameters model. """

for idx1 in range(len(X_embedded)):
  for idx2 in range(len(X_embedded)):
    # if idx1 % 1000 == 0 and idx1 != 0:
    #   print('Iteration: ', idx1)
    # calculate distances between point
    sample1 = X_embedded[idx1]
    sample2 = X_embedded[idx2]
    distances_points[idx1][idx2] = np.sqrt(np.sum((sample1 - sample2) ** 2))

distances_points

print('Min distance between points: ', np.min(distances_points))
print('Max distance between points: ', np.max(distances_points))
print('Mean distance between points: ', np.mean(distances_points))

print('n-clusters, distance:')
for dist in np.linspace(4000, 15000, 25):
  dbscan = DBSCAN(eps=dist).fit(X_embedded)
  n_clusters = np.max(dbscan.labels_)
  print(n_clusters, dist)

"""We choose the epsilon parameter to be equal to 13008, respectively 4916 because we want to have 5 clusters (number of clusters found with the Elbow method)."""

dbscan = DBSCAN(eps=4916)
dbscan.fit(X_embedded)
predicted_labels_train = dbscan.labels_
n_clusters = np.max(dbscan.labels_) + 1

n_clusters

cluster_samples = {}
X_train_ = np.array(X_embedded)
for i in range(n_clusters):
    cluster_samples[i] = X_train_[predicted_labels_train == i]
# cluster_samples

test_predictions = []
for sample in np.array(X_embedded_test):
    min_distances = []
    for cluster_n in range(n_clusters):
        distances = np.sum((cluster_samples[cluster_n] - sample) ** 2, axis = 1)
        min_distance = np.min(distances)
        min_distances.append(min_distance)
    test_predictions.append(np.argmin(min_distances))
test_predictions = np.array(test_predictions)

confusion_matrix = np.zeros((n_clusters, n_clusters))

for idx, i in enumerate(y_train):
    for idx2, j in enumerate(predicted_labels_train):
        if idx == idx2:
          confusion_matrix[i][j] += 1

print('Confusion matrix:\n', confusion_matrix)


print(np.mean(test_predictions == y_test))

"""Evaluate DBSCAN model"""

_extremely_severe_depression = [ind for ind, value in enumerate(test_predictions) if value == 4]
print(len(_extremely_severe_depression))

items_dass_scales['Depression']

X_test_ = X_test.iloc[_extremely_severe_depression[:10],:]
X_test_

key='Depression'
for item in items_dass_scales['Depression']:
  X_test_[key] = (X_test_.filter(regex='Q(%s)A' % '|'.join(map(str, item))) - 1).sum(axis=1)
  
  bins = scores_bins_scales_dass[key] 
  bins.append((scores_bins_scales_dass[key][-1][1], X_test_[key].max() + 1))
  bins = pd.IntervalIndex.from_tuples(bins, closed='left')
  X_test_[key + '_category'] = np.arange(len(bins))[pd.cut(X_test_[key], bins=bins).cat.codes]

dass_scores = X_test_[items_dass_scales.keys()]
dass_categories = X_test_[[key + '_category' for key in items_dass_scales.keys()]]

