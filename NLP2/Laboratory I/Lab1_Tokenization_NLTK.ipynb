{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZY2NYnvgo4_"
      },
      "source": [
        "# NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlFVOZfTgo5D"
      },
      "source": [
        "#### Install NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mLQx904Fgo5E",
        "outputId": "e43153b7-8a8b-4dcb-89a1-37734ef29828",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CciCitngo5G"
      },
      "source": [
        "#### Download models or corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E0_cVKKxgo5H",
        "outputId": "0387b2a2-3a68-41d3-e667-6115c51cca84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: brown\n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "Hit Enter to continue: \n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "Hit Enter to continue: \n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "Hit Enter to continue: punkt\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "Hit Enter to continue: treebank\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet2021......... Open English Wordnet 2021\n",
            "  [ ] wordnet31........... Wordnet 3.1\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "Hit Enter to continue: \n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> x\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        }
      ],
      "source": [
        "!python -m nltk.downloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS44955ngo5H"
      },
      "source": [
        "#### Import and use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZLkk0M5fgo5I",
        "outputId": "698e1f3d-dccb-4057-e10c-c1d83b037483",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6NsrTsV9go5J"
      },
      "outputs": [],
      "source": [
        "tweet = \"RT @lOR42wsOEFcv3f: I fall too fast, crash too hard, forgive too easily and care too much... :( #amiright\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5820w6M1go5K"
      },
      "outputs": [],
      "source": [
        "# query = 'fast'\n",
        "query = 'are'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fm0uSkRYgo5K",
        "outputId": "bbe9553a-3f31-4e0c-ab36-9023faaeff59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "77"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tweet.find(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeERi57ggo5L"
      },
      "source": [
        "#### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "p0QRcjk-go5L",
        "outputId": "b5cb5f80-d00d-4356-ed0e-2afe22783443",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " '@lOR42wsOEFcv3f:',\n",
              " 'I',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast,',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard,',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much...',\n",
              " ':(',\n",
              " '#amiright']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "tweet.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SfXWnbsTgo5M",
        "outputId": "d27a6a77-725f-4ac4-e7af-33343410df09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[False]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "[\"fast\" in tweet.split()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dYPgmMRDgo5M",
        "outputId": "da85192e-6f6c-4bcf-ef2f-a34c5cd0ecc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " '@',\n",
              " 'lOR42wsOEFcv3f',\n",
              " ':',\n",
              " 'I',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " ',',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " ',',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':',\n",
              " '(',\n",
              " '#',\n",
              " 'amiright']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "nltk.word_tokenize(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gu305xZVgo5N",
        "outputId": "def20016-e863-41b7-fc65-5431f00ae902",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[False]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "[query in nltk.word_tokenize(tweet)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bICONsgugo5N",
        "outputId": "107842e5-9729-4225-e8b6-d37400d7bad1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " '@',\n",
              " 'lOR42wsOEFcv3f',\n",
              " ':',\n",
              " 'I',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " ',',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " ',',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':',\n",
              " '(',\n",
              " '#',\n",
              " 'amiright']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "nltk.word_tokenize(tweet, language='spanish')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.word_tokenize(\"We aren't here\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFgtszj-Zlep",
        "outputId": "d92531e9-0e7f-4a64-d3b7-e153c5f71b7b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We', 'are', \"n't\", 'here']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vwOIm8Ppgo5O"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "custom_tokenizer = RegexpTokenizer('[a-zA-Z0-9]*', discard_empty=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OOS9SBtdgo5O",
        "outputId": "654faf2b-f7a5-4dd0-ec34-50b5cc2af563",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " '',\n",
              " '',\n",
              " 'lOR42wsOEFcv3f',\n",
              " '',\n",
              " '',\n",
              " 'I',\n",
              " '',\n",
              " 'fall',\n",
              " '',\n",
              " 'too',\n",
              " '',\n",
              " 'fast',\n",
              " '',\n",
              " '',\n",
              " 'crash',\n",
              " '',\n",
              " 'too',\n",
              " '',\n",
              " 'hard',\n",
              " '',\n",
              " '',\n",
              " 'forgive',\n",
              " '',\n",
              " 'too',\n",
              " '',\n",
              " 'easily',\n",
              " '',\n",
              " 'and',\n",
              " '',\n",
              " 'care',\n",
              " '',\n",
              " 'too',\n",
              " '',\n",
              " 'much',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'amiright',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "custom_tokenizer.tokenize(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jS4Ujswqgo5P"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "odSRKHsXgo5P",
        "outputId": "a737db0b-14b9-409f-b71d-c0135892b31a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " ':',\n",
              " 'I',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " ',',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " ',',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(',\n",
              " '#amiright']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "tweet_tokenizer.tokenize(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_tokenizer.tokenize(\"We aren't here\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRvZi4zBZzcQ",
        "outputId": "6c10c6fc-886f-40ac-9f07-e94e9bbf5d26"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We', \"aren't\", 'here']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2pr5Invxgo5P",
        "outputId": "18c7c726-5128-4487-ba61-f78157dbb4ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " ':',\n",
              " 'I',\n",
              " 'fall',\n",
              " 'too_fast',\n",
              " ',',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " ',',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(',\n",
              " '#amiright']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "mwe = MWETokenizer()\n",
        "mwe.add_mwe(('too', 'fast'))\n",
        "mwe.tokenize(tweet_tokenizer.tokenize(tweet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "P1A2iQsRgo5Q"
      },
      "outputs": [],
      "source": [
        "mwe.add_mwe((('too', 'fast'), ('too', 'hard')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KPvhO_k0go5Q",
        "outputId": "1074b4f5-ce2b-4410-d0d5-51642c941d24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "query = 'fast'\n",
        "query in mwe.tokenize(tweet_tokenizer.tokenize(tweet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r212dFcgo5R"
      },
      "source": [
        "### Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LaZbZf3tgo5R",
        "outputId": "545f09e9-0ab2-436c-bf8a-11ab8945cf86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'rt @lor42wsoefcv3f: i fall too fast, crash too hard, forgive too easily and care too much... :( #amiright'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "tweet.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4REb4QpCgo5R"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def normalize_tokens(tokenized_text):\n",
        "    # Lowercase\n",
        "    tokens = [t.lower() for t in tokenized_text]\n",
        "    # Remove hashtags\n",
        "    tokens = [t for t in tokens if not t.startswith('#')]\n",
        "    # Remove punctuation\n",
        "    tokens = [t for t in tokens if t not in string.punctuation]\n",
        "    # Keep only letters\n",
        "#     tokens = [t for t in tokens if re.match('^[a-z]+$', t)]\n",
        "    # Normalize characters\n",
        "#     tokens = [re.sub('á', 'a', t) for t in tokens]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Kt-MiwD5go5S",
        "outputId": "91b0b854-e9e9-4c34-cb23-d5bf33469f73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['muy', 'rápido']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "spanish_query = 'muy rápido'\n",
        "normalize_tokens(tweet_tokenizer.tokenize(spanish_query))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNKJX0WP4a0z",
        "outputId": "772376bb-cd2a-4f62-b384-c81b217f41c0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "bpQehX46go5S",
        "outputId": "8fca0608-c5c7-49d3-df79-9270f14d6a45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'muy rapido'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "import unidecode\n",
        "unidecode.unidecode(spanish_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "22vT0ccIgo5S",
        "outputId": "7f74c866-dbfa-4af0-d619-b1463d0afa58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt',\n",
              " 'i',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "normalize_tokens(tweet_tokenizer.tokenize(tweet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZbR5ovZgo5T"
      },
      "source": [
        "#### Uniform normalization principle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "d5wVD5Q9go5T",
        "outputId": "07bf9236-65d0-4674-bc0b-53022dd133cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['too', 'fast', 'too', 'furious']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "query = 'TOO fast TOO furious'\n",
        "tokenized_query = tweet_tokenizer.tokenize(query)\n",
        "normalized_query = normalize_tokens(tokenized_query)\n",
        "# normalized_query = tokenized_query\n",
        "normalized_query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "qGCVOeOEgo5T",
        "outputId": "beace590-4a99-47e2-f246-bef310d2ed36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt',\n",
              " 'i',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "normalized_tweet = normalize_tokens(tweet_tokenizer.tokenize(tweet))\n",
        "# normalized_tweet = normalize_tokens(tweet.split())\n",
        "normalized_tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6msS2JqAgo5T",
        "outputId": "b3fcadad-d134-4302-e3b5-b99885027b7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fast', 'too'}\n",
            "2 common word(s)\n"
          ]
        }
      ],
      "source": [
        "common_words = set(normalized_query).intersection(normalized_tweet)\n",
        "print(common_words)\n",
        "print(len(common_words), \"common word(s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MksTMBo9go5U"
      },
      "source": [
        "#### Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "JkfEuEcIgo5U",
        "outputId": "c1739f40-a25e-4a41-b160-570876fcf0fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "KVI5F1shgo5U",
        "outputId": "671c1e61-5144-4051-f965-c7bef809a20e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'abia',\n",
              " 'acea',\n",
              " 'aceasta',\n",
              " 'această',\n",
              " 'aceea',\n",
              " 'aceeasi',\n",
              " 'acei',\n",
              " 'aceia',\n",
              " 'acel',\n",
              " 'acela',\n",
              " 'acelasi',\n",
              " 'acele',\n",
              " 'acelea',\n",
              " 'acest',\n",
              " 'acesta',\n",
              " 'aceste',\n",
              " 'acestea',\n",
              " 'acestei',\n",
              " 'acestia',\n",
              " 'acestui',\n",
              " 'aceşti',\n",
              " 'aceştia',\n",
              " 'adica',\n",
              " 'ai',\n",
              " 'aia',\n",
              " 'aibă',\n",
              " 'aici',\n",
              " 'al',\n",
              " 'ala',\n",
              " 'ale',\n",
              " 'alea',\n",
              " 'alt',\n",
              " 'alta',\n",
              " 'altceva',\n",
              " 'altcineva',\n",
              " 'alte',\n",
              " 'altfel',\n",
              " 'alti',\n",
              " 'altii',\n",
              " 'altul',\n",
              " 'am',\n",
              " 'anume',\n",
              " 'apoi',\n",
              " 'ar',\n",
              " 'are',\n",
              " 'as',\n",
              " 'asa',\n",
              " 'asta',\n",
              " 'astea',\n",
              " 'astfel',\n",
              " 'asupra',\n",
              " 'atare',\n",
              " 'atat',\n",
              " 'atata',\n",
              " 'atatea',\n",
              " 'atatia',\n",
              " 'ati',\n",
              " 'atit',\n",
              " 'atita',\n",
              " 'atitea',\n",
              " 'atitia',\n",
              " 'atunci',\n",
              " 'au',\n",
              " 'avea',\n",
              " 'avem',\n",
              " 'aveţi',\n",
              " 'avut',\n",
              " 'aş',\n",
              " 'aţi',\n",
              " 'ba',\n",
              " 'ca',\n",
              " 'cam',\n",
              " 'cand',\n",
              " 'care',\n",
              " 'careia',\n",
              " 'carora',\n",
              " 'caruia',\n",
              " 'cat',\n",
              " 'catre',\n",
              " 'ce',\n",
              " 'cea',\n",
              " 'ceea',\n",
              " 'cei',\n",
              " 'ceilalti',\n",
              " 'cel',\n",
              " 'cele',\n",
              " 'celor',\n",
              " 'ceva',\n",
              " 'chiar',\n",
              " 'ci',\n",
              " 'cind',\n",
              " 'cine',\n",
              " 'cineva',\n",
              " 'cit',\n",
              " 'cita',\n",
              " 'cite',\n",
              " 'citeva',\n",
              " 'citi',\n",
              " 'citiva',\n",
              " 'cu',\n",
              " 'cui',\n",
              " 'cum',\n",
              " 'cumva',\n",
              " 'cât',\n",
              " 'câte',\n",
              " 'câtva',\n",
              " 'câţi',\n",
              " 'cînd',\n",
              " 'cît',\n",
              " 'cîte',\n",
              " 'cîtva',\n",
              " 'cîţi',\n",
              " 'că',\n",
              " 'căci',\n",
              " 'cărei',\n",
              " 'căror',\n",
              " 'cărui',\n",
              " 'către',\n",
              " 'da',\n",
              " 'daca',\n",
              " 'dacă',\n",
              " 'dar',\n",
              " 'dat',\n",
              " 'dată',\n",
              " 'dau',\n",
              " 'de',\n",
              " 'deasupra',\n",
              " 'deci',\n",
              " 'decit',\n",
              " 'deja',\n",
              " 'desi',\n",
              " 'despre',\n",
              " 'deşi',\n",
              " 'din',\n",
              " 'dintr',\n",
              " 'dintr-',\n",
              " 'dintre',\n",
              " 'doar',\n",
              " 'doi',\n",
              " 'doilea',\n",
              " 'două',\n",
              " 'drept',\n",
              " 'dupa',\n",
              " 'după',\n",
              " 'dă',\n",
              " 'e',\n",
              " 'ea',\n",
              " 'ei',\n",
              " 'el',\n",
              " 'ele',\n",
              " 'era',\n",
              " 'eram',\n",
              " 'este',\n",
              " 'eu',\n",
              " 'eşti',\n",
              " 'face',\n",
              " 'fara',\n",
              " 'fata',\n",
              " 'fel',\n",
              " 'fi',\n",
              " 'fie',\n",
              " 'fiecare',\n",
              " 'fii',\n",
              " 'fim',\n",
              " 'fiu',\n",
              " 'fiţi',\n",
              " 'foarte',\n",
              " 'fost',\n",
              " 'fără',\n",
              " 'i',\n",
              " 'ia',\n",
              " 'iar',\n",
              " 'ii',\n",
              " 'il',\n",
              " 'imi',\n",
              " 'in',\n",
              " 'inainte',\n",
              " 'inapoi',\n",
              " 'inca',\n",
              " 'incit',\n",
              " 'insa',\n",
              " 'intr',\n",
              " 'intre',\n",
              " 'isi',\n",
              " 'iti',\n",
              " 'la',\n",
              " 'le',\n",
              " 'li',\n",
              " 'lor',\n",
              " 'lui',\n",
              " 'lângă',\n",
              " 'lîngă',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'mai',\n",
              " 'mea',\n",
              " 'mei',\n",
              " 'mele',\n",
              " 'mereu',\n",
              " 'meu',\n",
              " 'mi',\n",
              " 'mie',\n",
              " 'mine',\n",
              " 'mod',\n",
              " 'mult',\n",
              " 'multa',\n",
              " 'multe',\n",
              " 'multi',\n",
              " 'multă',\n",
              " 'mulţi',\n",
              " 'mâine',\n",
              " 'mîine',\n",
              " 'mă',\n",
              " 'ne',\n",
              " 'ni',\n",
              " 'nici',\n",
              " 'nimeni',\n",
              " 'nimic',\n",
              " 'niste',\n",
              " 'nişte',\n",
              " 'noastre',\n",
              " 'noastră',\n",
              " 'noi',\n",
              " 'nostri',\n",
              " 'nostru',\n",
              " 'nou',\n",
              " 'noua',\n",
              " 'nouă',\n",
              " 'noştri',\n",
              " 'nu',\n",
              " 'numai',\n",
              " 'o',\n",
              " 'or',\n",
              " 'ori',\n",
              " 'oricare',\n",
              " 'orice',\n",
              " 'oricine',\n",
              " 'oricum',\n",
              " 'oricând',\n",
              " 'oricât',\n",
              " 'oricînd',\n",
              " 'oricît',\n",
              " 'oriunde',\n",
              " 'pai',\n",
              " 'parca',\n",
              " 'patra',\n",
              " 'patru',\n",
              " 'pe',\n",
              " 'pentru',\n",
              " 'peste',\n",
              " 'pic',\n",
              " 'pina',\n",
              " 'poate',\n",
              " 'pot',\n",
              " 'prea',\n",
              " 'prima',\n",
              " 'primul',\n",
              " 'prin',\n",
              " 'printr-',\n",
              " 'putini',\n",
              " 'puţin',\n",
              " 'puţina',\n",
              " 'puţină',\n",
              " 'până',\n",
              " 'pînă',\n",
              " 'sa',\n",
              " 'sa-mi',\n",
              " 'sa-ti',\n",
              " 'sai',\n",
              " 'sale',\n",
              " 'sau',\n",
              " 'se',\n",
              " 'si',\n",
              " 'sint',\n",
              " 'sintem',\n",
              " 'spate',\n",
              " 'spre',\n",
              " 'sub',\n",
              " 'sunt',\n",
              " 'suntem',\n",
              " 'sunteţi',\n",
              " 'sus',\n",
              " 'să',\n",
              " 'săi',\n",
              " 'său',\n",
              " 't',\n",
              " 'ta',\n",
              " 'tale',\n",
              " 'te',\n",
              " 'ti',\n",
              " 'tine',\n",
              " 'toata',\n",
              " 'toate',\n",
              " 'toată',\n",
              " 'tocmai',\n",
              " 'tot',\n",
              " 'toti',\n",
              " 'totul',\n",
              " 'totusi',\n",
              " 'totuşi',\n",
              " 'toţi',\n",
              " 'trei',\n",
              " 'treia',\n",
              " 'treilea',\n",
              " 'tu',\n",
              " 'tuturor',\n",
              " 'tăi',\n",
              " 'tău',\n",
              " 'u',\n",
              " 'ul',\n",
              " 'ului',\n",
              " 'un',\n",
              " 'una',\n",
              " 'unde',\n",
              " 'undeva',\n",
              " 'unei',\n",
              " 'uneia',\n",
              " 'unele',\n",
              " 'uneori',\n",
              " 'unii',\n",
              " 'unor',\n",
              " 'unora',\n",
              " 'unu',\n",
              " 'unui',\n",
              " 'unuia',\n",
              " 'unul',\n",
              " 'v',\n",
              " 'va',\n",
              " 'vi',\n",
              " 'voastre',\n",
              " 'voastră',\n",
              " 'voi',\n",
              " 'vom',\n",
              " 'vor',\n",
              " 'vostru',\n",
              " 'vouă',\n",
              " 'voştri',\n",
              " 'vreo',\n",
              " 'vreun',\n",
              " 'vă',\n",
              " 'zi',\n",
              " 'zice',\n",
              " 'îi',\n",
              " 'îl',\n",
              " 'îmi',\n",
              " 'în',\n",
              " 'îţi',\n",
              " 'ăla',\n",
              " 'ălea',\n",
              " 'ăsta',\n",
              " 'ăstea',\n",
              " 'ăştia',\n",
              " 'şi',\n",
              " 'ţi',\n",
              " 'ţie']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.words('romanian')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ChfsRB90go5U"
      },
      "outputs": [],
      "source": [
        "blacklist_words = stopwords.words('english') + ['rt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "rH-ekGL3go5V",
        "outputId": "6a804a68-0b95-443f-d271-30c691684a90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fall', 'fast', 'crash', 'hard', 'forgive', 'easily', 'care', 'much', '...', ':(']\n"
          ]
        }
      ],
      "source": [
        "cleaned_tweet = [t for t in normalized_tweet if t not in blacklist_words]\n",
        "print(cleaned_tweet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L9GWe87go5V"
      },
      "source": [
        "#### Stemming / Lemmatization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "DwKmEiS9go5V"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WordNet and OpenMultilingualWordnet necessary for lemmatization\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "# Trained tagger needed for POS-tagging:\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmrctMmM4vxn",
        "outputId": "a718ddbc-5660-4111-e77d-91fd70f35698"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "PMrjEi_wgo5V",
        "outputId": "c470b953-c4fc-49db-920d-b8f975177287",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fall',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'hard',\n",
              " 'forgiv',\n",
              " 'easili',\n",
              " 'care',\n",
              " 'much',\n",
              " '...',\n",
              " ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "[stemmer.stem(t) for t in cleaned_tweet]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "QGS64z4Sgo5W",
        "outputId": "81f7cb42-c6b9-4c09-bc62-cd243691608b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fall',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'hard',\n",
              " 'forgiv',\n",
              " 'easili',\n",
              " 'care',\n",
              " 'much',\n",
              " '...',\n",
              " ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "[stemmer.stem(t) for t in cleaned_tweet]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "bPJdL7efgo5W",
        "outputId": "df263b26-f876-49c7-eed7-ccd4c3b44c5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fall',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'hard',\n",
              " 'forgive',\n",
              " 'easily',\n",
              " 'care',\n",
              " 'much',\n",
              " '...',\n",
              " ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "[lemmatizer.lemmatize(t) for t in cleaned_tweet]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "gt9pUfdxgo5W",
        "outputId": "049e3023-8a1b-4aae-d012-c2c721eb5354",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('fall', 'NN'), ('fast', 'RB'), ('crash', 'JJ'), ('hard', 'JJ'), ('forgive', 'NN'), ('easily', 'RB'), ('care', 'VB'), ('much', 'JJ'), ('...', ':'), (':(', 'NN')]\n"
          ]
        }
      ],
      "source": [
        "tagged_tweet = nltk.pos_tag(cleaned_tweet)\n",
        "print(tagged_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "B0QR4GB6go5W"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "tag_map = {'J': wn.ADJ, 'V': wn.VERB, 'R': wn.ADV, 'N': wn.NOUN}\n",
        "def get_lemmas(tokenized_text):\n",
        "    tagged_text = nltk.pos_tag(tokenized_text)\n",
        "    return [lemmatizer.lemmatize(w, pos=tag_map.get(p[0], wn.NOUN)) for (w, p) in tagged_text]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "b9Tftzy_go5X",
        "outputId": "4d5a682e-c442-41db-9052-f85f0faa58ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'fastest']\n"
          ]
        }
      ],
      "source": [
        "query = \"the fastest!\"\n",
        "normalized_query = normalize_tokens(tweet_tokenizer.tokenize(query))\n",
        "print(normalized_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "cc7OAzvdgo5X",
        "outputId": "39ba3e39-6bd7-4af6-d94d-09f61c87763c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['rt', 'i', 'fall', 'too', 'fast', 'crash', 'too', 'hard', 'forgive', 'too', 'easily', 'and', 'care', 'too', 'much', '...', ':(']\n",
            "['the', 'fast']\n"
          ]
        }
      ],
      "source": [
        "lemmatized_tweet = get_lemmas(normalized_tweet)\n",
        "lemmatized_query = get_lemmas(normalized_query)\n",
        "print(lemmatized_tweet)\n",
        "print(lemmatized_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "mjqRkPSdgo5X",
        "outputId": "13becd3f-bc14-46aa-bcc8-922f6284eb78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'be', 'so', 'fast', 'i', 'be', 'the', 'fast']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "tweet = \"I am so fast, I am the fastest!\"\n",
        "normalized_tweet = normalize_tokens(tweet_tokenizer.tokenize(tweet))\n",
        "# normalized_tweet\n",
        "# [lemmatizer.lemmatize(t) for t in normalized_tweet]\n",
        "get_lemmas(normalized_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "-GvvkXv9go5X",
        "outputId": "58afc5c1-afb8-427c-c711-748c1dfd4a58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common words: {'fast'}\n"
          ]
        }
      ],
      "source": [
        "print(\"Common words:\", set(lemmatized_tweet).intersection(set(lemmatized_query)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "S1iUJDoAgo5X",
        "outputId": "b6455c90-b1cb-46eb-bafe-f2d1662ad673",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fall',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'hard',\n",
              " 'forgive',\n",
              " 'easily',\n",
              " 'care',\n",
              " 'much',\n",
              " '...',\n",
              " ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "get_lemmas(cleaned_tweet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4vdMqDMgo5Y"
      },
      "source": [
        "#### Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "0uK26uflgo5Y",
        "outputId": "859afaf2-b81d-4572-9fce-2f0c7f3fefd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 2), ('be', 2), ('fast', 2), ('so', 1), ('the', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "Counter(get_lemmas(normalized_tweet)).most_common(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "VM29Jm79go5Y",
        "outputId": "17430289-9525-4965-97c1-117f8e1f8ed3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'be', 'so', 'fast', 'i', 'be', 'the', 'fast']\n"
          ]
        }
      ],
      "source": [
        "tweet = \"I am so fast, I am the fastest!\"\n",
        "normalized_tweet = normalize_tokens(tweet_tokenizer.tokenize(tweet))\n",
        "lemmatized_tweet = get_lemmas(normalized_tweet)\n",
        "print(lemmatized_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "21ETm-FXgo5Y",
        "outputId": "b2b420e4-0539-42b5-f94c-458319dff005",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'i': 2, 'am': 2, 'so': 1, 'fast': 1, 'the': 1, 'fastest': 1})\n",
            "Counter({'i': 2, 'be': 2, 'fast': 2, 'so': 1, 'the': 1})\n"
          ]
        }
      ],
      "source": [
        "print(Counter(normalized_tweet))\n",
        "print(Counter(lemmatized_tweet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDv67B02go5Z"
      },
      "source": [
        "#### Sentence segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "6ig3gIxugo5Z"
      },
      "outputs": [],
      "source": [
        "query = \"I am too fast. I am too furious.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "mIUrh4YRgo5Z"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "CweNmFNkgo5Z",
        "outputId": "39dd5f87-b802-4433-8615-b63666e9bb70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I am too fast.', 'I am too furious.']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "sent_tokenize(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Rkq4yB_-go5Z",
        "outputId": "2dc831e1-ce0b-4464-a1c1-1391ababa950",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Soy muy rápido!', 'Estoy muy furioso!']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')\n",
        "spanish_query = 'Soy muy rápido! Estoy muy furioso!'\n",
        "spanish_tokenizer.tokenize(spanish_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "66jVuyn3go5a",
        "outputId": "3073625b-7251-4d8c-9875-d7c9fece8602",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['J.K. Rowling is rich.', 'I am not as rich as J.K.']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "sent_tokenize(\"J.K. Rowling is rich. I am not as rich as J.K.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Fv-IkfQkgo5a"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "PunktSentenceTokenizer??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVOloB3Wgo5a"
      },
      "source": [
        "#### Numericalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "tS5LxLJ_go5a",
        "outputId": "f2139762-f9c7-4ffd-932b-9a84f4b13c1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['stay', 'tune']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "get_lemmas(normalize_tokens(custom_tokenizer.tokenize(\"STAY TUNED!\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises"
      ],
      "metadata": {
        "id": "4M6aE4Q5wcdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find a tweet / text posted recently on social media in a language different than English. Write a function `normalize_tokens()` that receives as input a list of tokens and is appropriate for normalizing the tokens in the input text (possibly handle special characters, stopwords for the chosen language etc). \n",
        "\n",
        "Split the text into sentences, then tokenize the text using one of the tokenizers introduced above, then apply the implemented function.  Did the provided tools offer enough support / which problems did you encounter for specific languages?"
      ],
      "metadata": {
        "id": "Nal_3d72weVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "french_tweet = \"Manifester chez les gens, c'est mal. \" \\\n",
        "                + \"Eh bien, les Français jettent des éviers de cuisine par la fenêtre de leur président quand \" \\\n",
        "                + \"ils augmentent le prix de l'essence de 10 cents, et ils ont aussi des soins de santé gratuits, \" \\\n",
        "                + \"alors je ne suis pas d'accord.\""
      ],
      "metadata": {
        "id": "N7E4R8d0wdzX"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.tokenize.sent_tokenize(french_tweet, language='french')\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBZeA2KBkNof",
        "outputId": "4ef9e0a3-c4e5-4724-d013-736170553809"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Manifester chez les gens, c'est mal.\",\n",
              " \"Eh bien, les Français jettent des éviers de cuisine par la fenêtre de leur président quand ils augmentent le prix de l'essence de 10 cents, et ils ont aussi des soins de santé gratuits, alors je ne suis pas d'accord.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_tokenized = nltk.tokenize.word_tokenize(french_tweet, language='french')\n",
        "text_tokenized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulwklsAjkn59",
        "outputId": "c1aed4e8-1805-41b2-813b-fa18cb6985e3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Manifester',\n",
              " 'chez',\n",
              " 'les',\n",
              " 'gens',\n",
              " ',',\n",
              " \"c'est\",\n",
              " 'mal',\n",
              " '.',\n",
              " 'Eh',\n",
              " 'bien',\n",
              " ',',\n",
              " 'les',\n",
              " 'Français',\n",
              " 'jettent',\n",
              " 'des',\n",
              " 'éviers',\n",
              " 'de',\n",
              " 'cuisine',\n",
              " 'par',\n",
              " 'la',\n",
              " 'fenêtre',\n",
              " 'de',\n",
              " 'leur',\n",
              " 'président',\n",
              " 'quand',\n",
              " 'ils',\n",
              " 'augmentent',\n",
              " 'le',\n",
              " 'prix',\n",
              " 'de',\n",
              " \"l'essence\",\n",
              " 'de',\n",
              " '10',\n",
              " 'cents',\n",
              " ',',\n",
              " 'et',\n",
              " 'ils',\n",
              " 'ont',\n",
              " 'aussi',\n",
              " 'des',\n",
              " 'soins',\n",
              " 'de',\n",
              " 'santé',\n",
              " 'gratuits',\n",
              " ',',\n",
              " 'alors',\n",
              " 'je',\n",
              " 'ne',\n",
              " 'suis',\n",
              " 'pas',\n",
              " \"d'accord\",\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "french_stopwords = stopwords.words('french')\n",
        "french_stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx6kUfFtojhx",
        "outputId": "ffd0a389-c8c2-472b-9e32-739e4774f152"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['au',\n",
              " 'aux',\n",
              " 'avec',\n",
              " 'ce',\n",
              " 'ces',\n",
              " 'dans',\n",
              " 'de',\n",
              " 'des',\n",
              " 'du',\n",
              " 'elle',\n",
              " 'en',\n",
              " 'et',\n",
              " 'eux',\n",
              " 'il',\n",
              " 'ils',\n",
              " 'je',\n",
              " 'la',\n",
              " 'le',\n",
              " 'les',\n",
              " 'leur',\n",
              " 'lui',\n",
              " 'ma',\n",
              " 'mais',\n",
              " 'me',\n",
              " 'même',\n",
              " 'mes',\n",
              " 'moi',\n",
              " 'mon',\n",
              " 'ne',\n",
              " 'nos',\n",
              " 'notre',\n",
              " 'nous',\n",
              " 'on',\n",
              " 'ou',\n",
              " 'par',\n",
              " 'pas',\n",
              " 'pour',\n",
              " 'qu',\n",
              " 'que',\n",
              " 'qui',\n",
              " 'sa',\n",
              " 'se',\n",
              " 'ses',\n",
              " 'son',\n",
              " 'sur',\n",
              " 'ta',\n",
              " 'te',\n",
              " 'tes',\n",
              " 'toi',\n",
              " 'ton',\n",
              " 'tu',\n",
              " 'un',\n",
              " 'une',\n",
              " 'vos',\n",
              " 'votre',\n",
              " 'vous',\n",
              " 'c',\n",
              " 'd',\n",
              " 'j',\n",
              " 'l',\n",
              " 'à',\n",
              " 'm',\n",
              " 'n',\n",
              " 's',\n",
              " 't',\n",
              " 'y',\n",
              " 'été',\n",
              " 'étée',\n",
              " 'étées',\n",
              " 'étés',\n",
              " 'étant',\n",
              " 'étante',\n",
              " 'étants',\n",
              " 'étantes',\n",
              " 'suis',\n",
              " 'es',\n",
              " 'est',\n",
              " 'sommes',\n",
              " 'êtes',\n",
              " 'sont',\n",
              " 'serai',\n",
              " 'seras',\n",
              " 'sera',\n",
              " 'serons',\n",
              " 'serez',\n",
              " 'seront',\n",
              " 'serais',\n",
              " 'serait',\n",
              " 'serions',\n",
              " 'seriez',\n",
              " 'seraient',\n",
              " 'étais',\n",
              " 'était',\n",
              " 'étions',\n",
              " 'étiez',\n",
              " 'étaient',\n",
              " 'fus',\n",
              " 'fut',\n",
              " 'fûmes',\n",
              " 'fûtes',\n",
              " 'furent',\n",
              " 'sois',\n",
              " 'soit',\n",
              " 'soyons',\n",
              " 'soyez',\n",
              " 'soient',\n",
              " 'fusse',\n",
              " 'fusses',\n",
              " 'fût',\n",
              " 'fussions',\n",
              " 'fussiez',\n",
              " 'fussent',\n",
              " 'ayant',\n",
              " 'ayante',\n",
              " 'ayantes',\n",
              " 'ayants',\n",
              " 'eu',\n",
              " 'eue',\n",
              " 'eues',\n",
              " 'eus',\n",
              " 'ai',\n",
              " 'as',\n",
              " 'avons',\n",
              " 'avez',\n",
              " 'ont',\n",
              " 'aurai',\n",
              " 'auras',\n",
              " 'aura',\n",
              " 'aurons',\n",
              " 'aurez',\n",
              " 'auront',\n",
              " 'aurais',\n",
              " 'aurait',\n",
              " 'aurions',\n",
              " 'auriez',\n",
              " 'auraient',\n",
              " 'avais',\n",
              " 'avait',\n",
              " 'avions',\n",
              " 'aviez',\n",
              " 'avaient',\n",
              " 'eut',\n",
              " 'eûmes',\n",
              " 'eûtes',\n",
              " 'eurent',\n",
              " 'aie',\n",
              " 'aies',\n",
              " 'ait',\n",
              " 'ayons',\n",
              " 'ayez',\n",
              " 'aient',\n",
              " 'eusse',\n",
              " 'eusses',\n",
              " 'eût',\n",
              " 'eussions',\n",
              " 'eussiez',\n",
              " 'eussent']"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "def normalize_tokens(tokenized_text, stopwords=[]):\n",
        "  # Lowercasing\n",
        "  tokens = [t.lower() for t in tokenized_text]\n",
        "  # Remove punctuation\n",
        "  tokens = [t for t in tokens if t not in string.punctuation]\n",
        "  # Remove stopwords\n",
        "  tokens = [t for t in tokens if t not in stopwords]\n",
        "  # Normalize\n",
        "  tokens = [unicodedata.normalize('NFKD', t).encode('ascii', 'ignore').decode('utf-8', 'ignore') for t in tokens]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "normalized_text = ' '.join(normalize_tokens(text_tokenized, french_stopwords))\n",
        "print('Original text: ', french_tweet)\n",
        "print('Preprocessed text: ', normalized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rS0fWGvPk77w",
        "outputId": "8d9d88e9-3b22-41b4-d419-304b0e504f29"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:  Manifester chez les gens, c'est mal. Eh bien, les Français jettent des éviers de cuisine par la fenêtre de leur président quand ils augmentent le prix de l'essence de 10 cents, et ils ont aussi des soins de santé gratuits, alors je ne suis pas d'accord.\n",
            "Preprocessed text:  manifester chez gens , c'est mal . eh bien , francais jettent eviers cuisine fenetre president quand augmentent prix l'essence 10 cents , aussi soins sante gratuits , alors d'accord .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several low-resource languages, such as French, that lack adequate resources for NLP approaches. This is due the fact that foreign languages (other than English) lack linguistic knowledge, which can only be developed by specialists and native speakers, as well as the fact that it takes a huge amount of annotated data, which is frequently expensive to acquire."
      ],
      "metadata": {
        "id": "B5GcFjZQuiWm"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}