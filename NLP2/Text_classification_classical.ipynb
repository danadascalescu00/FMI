{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the 20newsgroups dataset using sklearn\n",
        "\n",
        "http://qwone.com/~jason/20Newsgroups/\n"
      ],
      "metadata": {
        "id": "TrrDo-CxCR0b"
      },
      "id": "TrrDo-CxCR0b"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "skilled-prime",
      "metadata": {
        "id": "skilled-prime"
      },
      "outputs": [],
      "source": [
        "# Loading the data set - annotated training data.\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "jewish-bankruptcy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jewish-bankruptcy",
        "outputId": "557d1503-0b1c-4bb3-81b3-0ff7a3a2c49b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# You can check the target names (categories) and some data files by following commands.\n",
        "twenty_train.target_names #prints all the categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "above-technical",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "above-technical",
        "outputId": "4e96167a-5fe7-4f89-dfd2-61d479724336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From: lerxst@wam.umd.edu (where's my thing)\n",
            "Subject: WHAT car is this!?\n",
            "Nntp-Posting-Host: rac3.wam.umd.edu\n",
            "Organization: University of Maryland, College Park\n",
            "Lines: 15\n",
            "\n",
            " I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "\n",
            "Thanks,\n",
            "- IL\n",
            "   ---- brought to you by your neighborhood Lerxst ----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:100])) #prints first 100 lines of the first data file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "twenty_train.target[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ7y8iS6SFZU",
        "outputId": "9af0e08d-994c-49bf-e7ab-d894567a67dd"
      },
      "id": "hQ7y8iS6SFZU",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(twenty_train.target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BJgUYOcVpOz",
        "outputId": "36945d16-7053-4d29-9e51-902f00de6175"
      },
      "id": "2BJgUYOcVpOz",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, n in enumerate(twenty_train.target_names):\n",
        "  print(i, n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBnj8IhRSJmd",
        "outputId": "405809a5-7dd4-4548-b4e2-6b508ac4cadf"
      },
      "id": "gBnj8IhRSJmd",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 alt.atheism\n",
            "1 comp.graphics\n",
            "2 comp.os.ms-windows.misc\n",
            "3 comp.sys.ibm.pc.hardware\n",
            "4 comp.sys.mac.hardware\n",
            "5 comp.windows.x\n",
            "6 misc.forsale\n",
            "7 rec.autos\n",
            "8 rec.motorcycles\n",
            "9 rec.sport.baseball\n",
            "10 rec.sport.hockey\n",
            "11 sci.crypt\n",
            "12 sci.electronics\n",
            "13 sci.med\n",
            "14 sci.space\n",
            "15 soc.religion.christian\n",
            "16 talk.politics.guns\n",
            "17 talk.politics.mideast\n",
            "18 talk.politics.misc\n",
            "19 talk.religion.misc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature extraction"
      ],
      "metadata": {
        "id": "dl7pImstCa18"
      },
      "id": "dl7pImstCa18"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "integral-order",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "integral-order",
        "outputId": "ff759799-486f-4d91-ef00-4beba2a03175"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Extracting features from text files\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer(max_features=10000)\n",
        "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
        "X_train_counts.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_vect.get_feature_names_out()[1000:2000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv9bxr5VZ2hv",
        "outputId": "283f06e3-1f8b-410f-fdf5-18f2b814e49f"
      },
      "id": "fv9bxr5VZ2hv",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['angles', 'angmar', 'angry', 'animal', 'animals', 'animated',\n",
              "       'animation', 'ankara', 'ann', 'announce', 'announced',\n",
              "       'announcement', 'annoyed', 'annoying', 'annual', 'anon',\n",
              "       'anonymity', 'anonymous', 'another', 'ansi', 'answer', 'answered',\n",
              "       'answering', 'answers', 'antenna', 'anthony', 'anti',\n",
              "       'antibiotics', 'antonio', 'anwar', 'any', 'anybody', 'anymore',\n",
              "       'anyone', 'anything', 'anytime', 'anyway', 'anyways', 'anywhere',\n",
              "       'ao', 'aoj', 'aol', 'ap', 'apana', 'apart', 'apartment', 'apc',\n",
              "       'api', 'apollo', 'apologies', 'apologize', 'apostle', 'apostles',\n",
              "       'app', 'apparent', 'apparently', 'appeal', 'appeals', 'appear',\n",
              "       'appearance', 'appeared', 'appearing', 'appears', 'apple',\n",
              "       'applelink', 'apples', 'applicable', 'application', 'applications',\n",
              "       'applied', 'applies', 'apply', 'applying', 'appointed',\n",
              "       'appreciate', 'appreciated', 'appressian', 'approach',\n",
              "       'approaches', 'approaching', 'appropriate', 'approval', 'approve',\n",
              "       'approved', 'approx', 'approximately', 'apps', 'apr', 'april',\n",
              "       'apt', 'aq', 'ar', 'arab', 'arabic', 'arabs', 'arbitrary', 'arbor',\n",
              "       'arc', 'arcade', 'archer', 'archie', 'architecture', 'archive',\n",
              "       'archives', 'are', 'area', 'areas', 'aren', 'arena', 'arens',\n",
              "       'arf', 'arg', 'argc', 'argentina', 'argic', 'args', 'argue',\n",
              "       'argued', 'arguing', 'argument', 'arguments', 'argv', 'ariane',\n",
              "       'ariel', 'arise', 'arithmetic', 'arizona', 'arm', 'armed',\n",
              "       'armenia', 'armenian', 'armenians', 'armored', 'armory', 'arms',\n",
              "       'army', 'around', 'arranged', 'arrangement', 'arras', 'array',\n",
              "       'arrest', 'arrested', 'arrive', 'arrived', 'arrogance', 'arrogant',\n",
              "       'arromdee', 'art', 'arthur', 'article', 'articles', 'artificial',\n",
              "       'arts', 'as', 'asala', 'ascii', 'asd', 'ashley', 'ashok', 'aside',\n",
              "       'ask', 'asked', 'asking', 'asks', 'asleep', 'aspartame', 'aspect',\n",
              "       'aspects', 'aspi4dos', 'ass', 'assault', 'assembled', 'assembly',\n",
              "       'assert', 'assertion', 'assertions', 'assessment', 'asshole',\n",
              "       'assigned', 'assist', 'assistance', 'assistant', 'assists',\n",
              "       'associate', 'associated', 'associates', 'association', 'assume',\n",
              "       'assumed', 'assumes', 'assuming', 'assumption', 'assumptions',\n",
              "       'assure', 'assured', 'ast', 'astein', 'astro', 'astronaut',\n",
              "       'astronomical', 'astronomy', 'astros', 'asynchronous', 'at', 'ata',\n",
              "       'atari', 'atf', 'atheism', 'atheist', 'atheists', 'athena',\n",
              "       'athens', 'athos', 'ati', 'atlanta', 'atlantaga', 'atlantic',\n",
              "       'atlas', 'atm', 'atmosphere', 'atmospheric', 'atomic', 'atoms',\n",
              "       'atrocities', 'att', 'attach', 'attached', 'attack', 'attacked',\n",
              "       'attacking', 'attacks', 'attempt', 'attempted', 'attempting',\n",
              "       'attempts', 'attend', 'attendance', 'attended', 'attention',\n",
              "       'atterlep', 'attitude', 'attorney', 'attractive', 'attribute',\n",
              "       'attributed', 'attributes', 'au', 'auburn', 'audience', 'audio',\n",
              "       'aug', 'august', 'aurora', 'austin', 'australia', 'australian',\n",
              "       'austria', 'authentication', 'author', 'authorities', 'authority',\n",
              "       'authorization', 'authorized', 'authors', 'auto', 'autoexec',\n",
              "       'automated', 'automatic', 'automatically', 'automatics',\n",
              "       'automobile', 'automotive', 'autos', 'auvm', 'av', 'avail',\n",
              "       'availability', 'available', 'ave', 'avenue', 'average',\n",
              "       'averages', 'aviation', 'avoid', 'avoided', 'avoiding', 'aw',\n",
              "       'award', 'awards', 'aware', 'awareness', 'away', 'awd', 'awesome',\n",
              "       'awful', 'aws', 'ax', 'axe', 'axes', 'axis', 'ay', 'az',\n",
              "       'azerbaijan', 'azerbaijani', 'azerbaijanis', 'azeri', 'azeris',\n",
              "       'b0d', 'b1', 'b2', 'b30', 'b4q', 'b6', 'b8', 'b8e', 'b8f', 'b8g',\n",
              "       'b9', 'b9r', 'b_', 'ba', 'baalke', 'babb', 'babies', 'baby',\n",
              "       'babylon', 'bach', 'back', 'backed', 'background', 'backing',\n",
              "       'backup', 'bacteria', 'bad', 'badly', 'baerga', 'bag', 'bags',\n",
              "       'bailey', 'bake', 'baker', 'baku', 'balance', 'balanced',\n",
              "       'balcony', 'ball', 'balloon', 'balls', 'balltown', 'baltimore',\n",
              "       'ban', 'band', 'bands', 'bandwidth', 'bang', 'bank', 'banks',\n",
              "       'banned', 'banning', 'baptism', 'baptist', 'bar', 'barbara',\n",
              "       'bare', 'barely', 'barlow', 'barnes', 'barrel', 'barrier', 'barry',\n",
              "       'bars', 'base', 'baseball', 'based', 'basement', 'bases',\n",
              "       'bashing', 'basic', 'basically', 'basil', 'basis', 'basketball',\n",
              "       'bass', 'bassel', 'bastard', 'bat', 'batch', 'batf', 'bath',\n",
              "       'bathroom', 'batman', 'batteries', 'battery', 'batting', 'battle',\n",
              "       'baud', 'bauer', 'bay', 'bb', 'bbn', 'bbs', 'bc', 'bcstec', 'bd',\n",
              "       'be', 'beach', 'beam', 'bean', 'bear', 'bearing', 'bears', 'beast',\n",
              "       'beat', 'beaten', 'beating', 'beats', 'beauchaine', 'beautiful',\n",
              "       'beauty', 'beaverton', 'became', 'because', 'beck', 'beckman',\n",
              "       'become', 'becomes', 'becoming', 'bed', 'bedfellow', 'bedroom',\n",
              "       'been', 'beeps', 'beer', 'before', 'beg', 'began', 'begin',\n",
              "       'beginning', 'begins', 'begun', 'behalf', 'behanna', 'behave',\n",
              "       'behavior', 'behaviour', 'behind', 'being', 'beings', 'beleive',\n",
              "       'belfour', 'belief', 'beliefs', 'believe', 'believed', 'believer',\n",
              "       'believers', 'believes', 'believing', 'bell', 'bellcore',\n",
              "       'bellovin', 'belong', 'belongs', 'below', 'belt', 'ben', 'bench',\n",
              "       'benedikt', 'benefit', 'benefits', 'benjamin', 'bennett', 'bent',\n",
              "       'beretta', 'berkeley', 'berlin', 'bernard', 'bernoulli',\n",
              "       'bernstein', 'berry', 'berryhill', 'besides', 'best', 'bet',\n",
              "       'beta', 'beth', 'bethesda', 'better', 'between', 'betz', 'beware',\n",
              "       'beyer', 'beyond', 'bgsu', 'bh', 'bhj', 'bhjn', 'bi', 'bias',\n",
              "       'biased', 'bible', 'biblical', 'bibliography', 'bicycle', 'big',\n",
              "       'bigger', 'biggest', 'bih', 'bike', 'biker', 'bikers', 'bikes',\n",
              "       'bil', 'bill', 'billboard', 'billion', 'billions', 'bills',\n",
              "       'billy', 'bin', 'binah', 'binaries', 'binary', 'binding',\n",
              "       'bindings', 'binghamton', 'biochem', 'biochemistry', 'biological',\n",
              "       'biology', 'bios', 'bird', 'birth', 'birthday', 'bishop', 'bison',\n",
              "       'bissell', 'bit', 'bitmap', 'bitnet', 'bits', 'biz', 'bizarre',\n",
              "       'bj', 'bk', 'bl', 'black', 'blackhawks', 'blacks', 'blacksburg',\n",
              "       'blah', 'blame', 'blaming', 'blank', 'blanket', 'blast', 'blaze',\n",
              "       'bless', 'blessed', 'blew', 'blind', 'blindly', 'block', 'blocks',\n",
              "       'blood', 'bloody', 'bloom', 'blow', 'blowing', 'blown', 'blows',\n",
              "       'blue', 'blues', 'blvd', 'bm', 'bmd', 'bmp', 'bms', 'bmug', 'bmw',\n",
              "       'bn', 'bnr', 'bo', 'board', 'boards', 'boat', 'bob', 'bobbe',\n",
              "       'bobby', 'bodies', 'bodily', 'body', 'boeing', 'boggs', 'bogus',\n",
              "       'boi', 'boise', 'bold', 'bolshevik', 'bolt', 'bomb', 'bombers',\n",
              "       'bombing', 'bonds', 'bone', 'bones', 'bontchev', 'bony', 'bony1',\n",
              "       'book', 'books', 'bookstore', 'boom', 'boomer', 'boost', 'booster',\n",
              "       'boot', 'booth', 'boots', 'borden', 'border', 'borders', 'boring',\n",
              "       'borland', 'born', 'bos', 'bosnia', 'bosnian', 'boss', 'boston',\n",
              "       'both', 'bother', 'bothered', 'bothers', 'bottle', 'bottom',\n",
              "       'bought', 'boulder', 'bounce', 'bound', 'boundaries', 'boundary',\n",
              "       'bowling', 'bowman', 'box', 'boxes', 'boy', 'boyle', 'boys', 'bp',\n",
              "       'bps', 'bq', 'br', 'brad', 'bradford', 'bradley', 'brady', 'brain',\n",
              "       'brains', 'brake', 'brakes', 'braking', 'branch', 'brand',\n",
              "       'brandeis', 'brands', 'brandt', 'brave', 'braves', 'bread',\n",
              "       'break', 'breaker', 'breaking', 'breaks', 'breath', 'breathing',\n",
              "       'brendan', 'brent', 'breton', 'brett', 'brian', 'brick', 'brief',\n",
              "       'briefing', 'briefly', 'brigham', 'bright', 'brightness', 'brind',\n",
              "       'bring', 'bringing', 'brings', 'bristol', 'britain', 'british',\n",
              "       'broad', 'broadcast', 'broke', 'broken', 'bronx', 'brookline',\n",
              "       'brooks', 'brother', 'brothers', 'brought', 'broward', 'brown',\n",
              "       'bruce', 'bruins', 'brunel', 'bruno', 'brunswick', 'brute',\n",
              "       'bryan', 'bs', 'bs0t', 'bsd', 'bskendig', 'bsu', 'bt', 'btw', 'bu',\n",
              "       'bubblejet', 'buck', 'bucks', 'buddy', 'budget', 'buf', 'buffalo',\n",
              "       'buffer', 'bug', 'bugs', 'build', 'building', 'buildings', 'built',\n",
              "       'bulb', 'bull', 'bullet', 'bulletin', 'bullets', 'bullock',\n",
              "       'bullpen', 'bullshit', 'bumper', 'bunch', 'bundled', 'buphy',\n",
              "       'burden', 'bure', 'bureau', 'buried', 'burn', 'burnaby', 'burned',\n",
              "       'burning', 'burns', 'burnt', 'burst', 'bury', 'bus', 'buses',\n",
              "       'bush', 'business', 'businesses', 'busy', 'but', 'butt', 'button',\n",
              "       'buttons', 'buy', 'buyer', 'buying', 'buzz', 'bv', 'bw', 'bx',\n",
              "       'bxlt', 'bxn', 'bxom', 'by', 'bye', 'byler', 'byte', 'bytes',\n",
              "       'byu', 'bz', 'c0', 'c2', 'c4', 'c4u', 'c6', 'c650', 'c7', 'c8',\n",
              "       'c8v', 'c9', 'c_', 'ca', 'cabin', 'cabinet', 'cable', 'cables',\n",
              "       'cache', 'cacs', 'cactus', 'cad', 'cadre', 'cae', 'caf', 'cage',\n",
              "       'cages', 'cain', 'cal', 'calculate', 'calculations', 'calgary',\n",
              "       'caliber', 'calif', 'california', 'call', 'callback', 'called',\n",
              "       'calling', 'callison', 'calls', 'calm', 'calpoly', 'calstate',\n",
              "       'caltech', 'calvin', 'cam', 'cambridge', 'came', 'camelot',\n",
              "       'camera', 'cameras', 'camp', 'campaign', 'campaign92', 'campbell',\n",
              "       'camps', 'campus', 'can', 'canada', 'canadian', 'canadians',\n",
              "       'canadiens', 'canberra', 'cancer', 'candida', 'candidate',\n",
              "       'candidates', 'cannot', 'canon', 'canterbury', 'canucks', 'cap',\n",
              "       'capabilities', 'capability', 'capable', 'capacitor', 'capacity',\n",
              "       'cape', 'capital', 'capitalist', 'capitals', 'caps', 'captain',\n",
              "       'capture', 'captured', 'car', 'caralv', 'card', 'carderock',\n",
              "       'cardinal', 'cardinals', 'cards', 'care', 'career', 'careful',\n",
              "       'carefully', 'cares', 'carina', 'carl', 'carleton', 'carlos',\n",
              "       'carnegie', 'carol', 'carolina', 'caronni', 'carpenter', 'carr',\n",
              "       'carried', 'carrier', 'carriers', 'carries', 'carroll', 'carry',\n",
              "       'carrying', 'cars', 'carson', 'carter', 'cartridge', 'cartridges',\n",
              "       'casbah', 'case', 'cases', 'casey', 'cash', 'casper', 'cassette',\n",
              "       'cast', 'castle', 'casual', 'casualties', 'cat', 'catalog',\n",
              "       'catbyte', 'catch', 'catcher', 'catchers', 'categories',\n",
              "       'category', 'catholic', 'catholics', 'cats', 'caught', 'cause',\n",
              "       'caused', 'causes', 'causing', 'cb', 'cbc', 'cbnews', 'cbnewsh',\n",
              "       'cbnewsj', 'cbnewsl', 'cc', 'ccc', 'cci', 'ccit', 'cco', 'ccs',\n",
              "       'ccu', 'ccwf', 'cd', 'cdac', 'cdc', 'cdi', 'cdp', 'cdrom', 'cds',\n",
              "       'cdt', 'ce', 'cease', 'cec', 'cec1', 'celebrate', 'celebration',\n",
              "       'celestial', 'cell', 'cellar', 'cells', 'cellular', 'censorship',\n",
              "       'centaur', 'center', 'centerline', 'centers', 'central', 'centre',\n",
              "       'centris', 'cents', 'centuries', 'century', 'ceremonial',\n",
              "       'ceremony', 'cern', 'certain', 'certainly', 'certainty', 'ces',\n",
              "       'cf', 'cfa', 'ch', 'ch981', 'chain', 'chair', 'chairman',\n",
              "       'challenge', 'challenged', 'challenges', 'chamber', 'chambers',\n",
              "       'champaign', 'champion', 'champions', 'championships'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(twenty_train.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im9YiRpoSe2A",
        "outputId": "b2041c75-9f9c-47cf-cc11-46d426bb933c"
      },
      "id": "im9YiRpoSe2A",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bearing-pilot",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bearing-pilot",
        "outputId": "c25cf8d6-eb9b-45cb-9187-e08d01c2f983"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Alternatively: use TF-IDF features\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X_train_tfidf.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a machine learning model to classify the texts"
      ],
      "metadata": {
        "id": "uD2X6z_8DeUf"
      },
      "id": "uD2X6z_8DeUf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive Bayes"
      ],
      "metadata": {
        "id": "fc1YF0atDp5f"
      },
      "id": "fc1YF0atDp5f"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "baking-roots",
      "metadata": {
        "id": "baking-roots"
      },
      "outputs": [],
      "source": [
        "# Machine Learning\n",
        "# Training Naive Bayes (NB) classifier on training data.\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "colonial-produce",
      "metadata": {
        "id": "colonial-produce"
      },
      "outputs": [],
      "source": [
        "# Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
        "# The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
        "# We will be using the 'text_clf' going forward.\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
        "\n",
        "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "activated-chuck",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "activated-chuck",
        "outputId": "2598abfc-62d9-4bd6-8cb3-2423b74e0f8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7738980350504514"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Performance of NB Classifier\n",
        "import numpy as np\n",
        "\n",
        "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
        "predicted = text_clf.predict(twenty_test.data)\n",
        "np.mean(predicted == twenty_test.target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "coastal-plastic",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coastal-plastic",
        "outputId": "84fc8c7f-0337-487a-e232-e32ccf60c4f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7, 11,  0, ...,  9,  3, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "predicted"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM"
      ],
      "metadata": {
        "id": "VfXoYCkLDsBQ"
      },
      "id": "VfXoYCkLDsBQ"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "respected-territory",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "respected-territory",
        "outputId": "b4bd8cbd-bedf-47a5-c50f-41973451c578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:700: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8159851301115242"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Training Support Vector Machines - SVM and calculating its performance\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "text_clf_svm = Pipeline([('vect', CountVectorizer(max_features=20000)), ('tfidf', TfidfTransformer()),\n",
        "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=5, random_state=42))])\n",
        "\n",
        "text_clf_svm = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
        "\n",
        "# Predicting labels on test set\n",
        "predicted_svm = text_clf_svm.predict(twenty_test.data)\n",
        "# Measuring performance (which metric??)\n",
        "np.mean(predicted_svm == twenty_test.target)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature extraction variants"
      ],
      "metadata": {
        "id": "3mwe9KcdIHsZ"
      },
      "id": "3mwe9KcdIHsZ"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XNsoN5aIL27",
        "outputId": "22daedc0-b589-4c63-e356-5ab5953367ac"
      },
      "id": "8XNsoN5aIL27",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "split-gazette",
      "metadata": {
        "id": "split-gazette"
      },
      "outputs": [],
      "source": [
        "# Removing stop words\n",
        "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), \n",
        "                     ('clf', MultinomialNB())])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming words\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "\n",
        "class StemmedCountVectorizer(CountVectorizer):\n",
        "    def build_analyzer(self):\n",
        "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
        "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
        "    \n",
        "stemmed_count_vect = StemmedCountVectorizer(stop_words='english', max_features=10000)\n",
        "\n",
        "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect), ('tfidf', TfidfTransformer()), \n",
        "                             ('mnb', MultinomialNB(fit_prior=False))])\n",
        "\n",
        "text_mnb_stemmed = text_mnb_stemmed.fit(twenty_train.data, twenty_train.target)\n",
        "\n",
        "predicted_mnb_stemmed = text_mnb_stemmed.predict(twenty_test.data)\n",
        "\n",
        "np.mean(predicted_mnb_stemmed == twenty_test.target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-0FdbCdWIz8",
        "outputId": "26e631ca-663e-458e-9f8c-66ed42086db4"
      },
      "id": "I-0FdbCdWIz8",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8138608603292619"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_mnb_stemmed['vect'].get_feature_names_out()[-1000:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_ZDKid9W3gN",
        "outputId": "54161444-e998-4931-b947-dacfc8c3d7dd"
      },
      "id": "a_ZDKid9W3gN",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['transmit', 'transmitt', 'transpar', 'transplant', 'transport',\n",
              "       'trap', 'trash', 'travel', 'travi', 'tray', 'tread', 'treasur',\n",
              "       'treasuri', 'treat', 'treati', 'treatment', 'tree', 'trek',\n",
              "       'tremend', 'trend', 'trentu', 'tri', 'trial', 'triangl',\n",
              "       'triangul', 'tribe', 'trick', 'tricki', 'trident', 'trigger',\n",
              "       'trillion', 'trim', 'triniti', 'trip', 'tripl', 'triumf',\n",
              "       'triumph', 'trivia', 'trivial', 'trol', 'troop', 'trophi',\n",
              "       'troubl', 'troy', 'truck', 'true', 'truecolor', 'truetyp', 'truli',\n",
              "       'trumpet', 'trunk', 'trust', 'truth', 'trw', 'ts', 'tsd', 'tseng',\n",
              "       'tsiel', 'tsn', 'tsr', 'tt', 'tti', 'ttl', 'ttu', 'tu', 'tube',\n",
              "       'tucson', 'tudelft', 'tue', 'tuesday', 'tuft', 'tuinstra',\n",
              "       'tulkarm', 'tulsa', 'tune', 'turbo', 'turgeon', 'turk', 'turkey',\n",
              "       'turkish', 'turkiy', 'turn', 'turner', 'turpin', 'tut', 'tutori',\n",
              "       'tv', 'tvtwm', 'tw', 'twice', 'twin', 'twist', 'twisto', 'twm',\n",
              "       'tx', 'txt', 'ty', 'type', 'typic', 'typo', 'tyranni', 'tyrant',\n",
              "       'tyre', 'tz', 'u0', 'u1', 'u2', 'u28037', 'u3', 'u34', 'u34u',\n",
              "       'u3l', 'u4', 'u5', 'u8', 'u_c', 'ua', 'uab', 'ualberta', 'uart',\n",
              "       'ub', 'ubc', 'ubvm', 'ubvmsb', 'uc', 'ucalgari', 'ucar', 'ucc',\n",
              "       'uccxkvb', 'ucdavi', 'ucf', 'uchicago', 'uci', 'ucla', 'uclink',\n",
              "       'ucsb', 'ucsc', 'ucsd', 'ucsu', 'ud', 'udel', 'uea', 'ufl', 'ufsa',\n",
              "       'ug', 'uga', 'ugli', 'uh', 'uhunix', 'uhura', 'ui', 'uic', 'uicvm',\n",
              "       'uidaho', 'uio', 'uiowa', 'uiuc', 'uj', 'uk', 'ukan', 'uki',\n",
              "       'ukrain', 'ul', 'ulf', 'ulkyvx', 'ulm', 'ulowel', 'ultb', 'ultim',\n",
              "       'ultra', 'ultrasound', 'ultraviolet', 'ultrix', 'ulyss', 'um',\n",
              "       'umanitoba', 'umass', 'umbc', 'umcc', 'umd', 'umich', 'umn',\n",
              "       'umpir', 'umr', 'unabl', 'unaccept', 'unarm', 'unassist',\n",
              "       'unauthor', 'unawar', 'unb', 'unbeliev', 'unbias', 'unbreak',\n",
              "       'unc', 'uncec', 'uncl', 'unclear', 'uncomfort', 'uncommon',\n",
              "       'uncondit', 'unconvent', 'uncov', 'und', 'undefin', 'undeni',\n",
              "       'under', 'undergo', 'undergrad', 'underground', 'underneath',\n",
              "       'understand', 'understood', 'undertak', 'undesir', 'undoubt',\n",
              "       'unemploy', 'unexpect', 'unfair', 'unfortun', 'unh', 'unhappi',\n",
              "       'uni', 'unicorn', 'unifi', 'uniform', 'union', 'uniqu', 'unisa',\n",
              "       'unisi', 'unisql', 'unit', 'uniti', 'univ', 'univers',\n",
              "       'universitaet', 'unix', 'unixg', 'unknown', 'unl', 'unlaw',\n",
              "       'unless', 'unlik', 'unlimit', 'unload', 'unlock', 'unlv', 'unm',\n",
              "       'unmoder', 'unnecessari', 'unnecessarili', 'unoc', 'unomaha',\n",
              "       'unorgan', 'unpleas', 'unplug', 'unreason', 'unregist', 'unsaf',\n",
              "       'unsign', 'unt', 'unto', 'unus', 'unusu', 'unwant', 'unwil',\n",
              "       'unworthi', 'unx', 'uo', 'uokmax', 'uoknor', 'uoregon', 'up',\n",
              "       'upcom', 'updat', 'upenn', 'upgrad', 'upi', 'upload', 'upper',\n",
              "       'upris', 'upset', 'upstair', 'uq', 'ur', 'uranium', 'uranus',\n",
              "       'urartu', 'urban', 'urbana', 'urg', 'urgent', 'ursa', 'usa',\n",
              "       'usabl', 'usaf', 'usag', 'usc', 'usd', 'use', 'useless', 'usenet',\n",
              "       'user', 'userag', 'usernam', 'usg', 'usl', 'usr', 'ussr', 'usu',\n",
              "       'usual', 'ut', 'uta', 'utah', 'utarlg', 'utc', 'utdalla', 'utexa',\n",
              "       'utica', 'util', 'utk', 'utkvm1', 'utkvx', 'utoledo', 'utoronto',\n",
              "       'utrecht', 'utsa', 'utter', 'utxvm', 'utzoo', 'uu', 'uucp',\n",
              "       'uudecod', 'uuencod', 'uug', 'uunet', 'uupcb', 'uv', 'uva', 'uvic',\n",
              "       'uw', 'uw0', 'uwa', 'uwaterloo', 'uwec', 'uwm', 'uwo', 'uwt',\n",
              "       'uww', 'uww7', 'ux', 'ux1', 'ux4', 'uxa', 'uxh', 'uy', 'uz', 'uzi',\n",
              "       'v0', 'v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v8', 'v9fq', 'va',\n",
              "       'va_list', 'vacat', 'vaccin', 'vacuum', 'vagina', 'vagu', 'vain',\n",
              "       'vak12', 'val', 'valentin', 'valeri', 'valid', 'valley', 'valu',\n",
              "       'valuabl', 'valv', 'van', 'vancouv', 'vandal', 'vanish', 'vari',\n",
              "       'variabl', 'variant', 'variat', 'varieti', 'various', 'vast',\n",
              "       'vat', 'vatican', 'vax', 'vax1', 'vax5', 'vaxc', 'vay', 'vb',\n",
              "       'vb30', 'vc', 'vcd', 'vcr', 'vcu', 'vd', 've', 'veal', 'vector',\n",
              "       'vega', 'veget', 'vegetarian', 'vehicl', 'vein', 'vela', 'velasco',\n",
              "       'veloc', 'venari', 'vendor', 'vent', 'ventur', 'ventura', 'venus',\n",
              "       'ver', 'vera', 'verbal', 'verdict', 'verif', 'verifi', 'vernon',\n",
              "       'vers', 'versa', 'version', 'versus', 'vertex', 'vertic', 'vesa',\n",
              "       'vessel', 'vesselin', 'vest', 'veteran', 'veto', 'vf', 'vg', 'vga',\n",
              "       'vh', 'vhf', 'vhs', 'vi', 'viabl', 'vibrat', 'vic', 'vice',\n",
              "       'victim', 'victor', 'victori', 'victoria', 'vida', 'video',\n",
              "       'videocart', 'videotap', 'vietnam', 'view', 'viewer', 'viewpoint',\n",
              "       'vike', 'vill', 'villag', 'vincent', 'vinyl', 'viola', 'violat',\n",
              "       'violenc', 'violent', 'violet', 'viper', 'vipunen', 'viral',\n",
              "       'virgilio', 'virgin', 'virginia', 'virtu', 'virtual', 'virus',\n",
              "       'vis', 'visa', 'visibl', 'vision', 'visit', 'visual', 'vital',\n",
              "       'vitamin', 'vl', 'vladimir', 'vladimirov', 'vlb', 'vlbi', 'vlsi',\n",
              "       'vm', 'vm1', 'vma', 'vmk', 'vms', 'vn', 'vnet', 'vnew', 'vo',\n",
              "       'voic', 'void', 'vojak', 'vol', 'volt', 'voltag', 'volum',\n",
              "       'volunt', 'voluntari', 'volvo', 'vomit', 'von', 'vos', 'vote',\n",
              "       'voter', 'vow', 'voyag', 'vp', 'vpic46', 'vpnet', 'vr', 'vram',\n",
              "       'vs', 'vt', 'vu', 'vulcan', 'vulner', 'vus', 'vv', 'vw', 'vx',\n",
              "       'vxcrna', 'vz', 'w0', 'w1', 'w11', 'w1t', 'w1w', 'w2', 'w27',\n",
              "       'w3p', 'w3q', 'w3s', 'w4', 'w44', 'w45', 'w47', 'w4wg', 'w7', 'w8',\n",
              "       'w87', 'wa', 'wa7', 'wacko', 'waco', 'wade', 'wage', 'wager',\n",
              "       'wagner', 'wagon', 'waii', 'waikato', 'wait', 'waiver', 'wake',\n",
              "       'wale', 'walk', 'walker', 'wall', 'walla', 'wallach', 'wallet',\n",
              "       'wallpap', 'walsh', 'walter', 'wam', 'wang', 'wanna', 'want',\n",
              "       'war', 'ward', 'ware', 'warfar', 'wari', 'warm', 'warn', 'warner',\n",
              "       'warp', 'warrant', 'warranti', 'warren', 'warrior', 'warsaw',\n",
              "       'warwick', 'wash', 'washer', 'washington', 'wasn', 'wast', 'watch',\n",
              "       'water', 'waterloo', 'watson', 'watstar', 'watt', 'wave', 'wax',\n",
              "       'way', 'wayn', 'wb', 'wc', 'wc_c', 'wcc', 'wcl', 'wcs', 'wd',\n",
              "       'wdstarr', 'weak', 'weaken', 'wealth', 'wealthi', 'weapon', 'wear',\n",
              "       'weather', 'weaver', 'web', 'webb', 'weber', 'webo', 'webster',\n",
              "       'wed', 'wednesday', 'week', 'weekend', 'weigh', 'weight', 'weird',\n",
              "       'weiss', 'weitek', 'welch', 'welcom', 'welfar', 'well', 'welti',\n",
              "       'went', 'weren', 'werner', 'wes', 'wesley', 'wesleyan', 'west',\n",
              "       'western', 'westminst', 'wet', 'wetwar', 'wfan', 'wg2', 'wh',\n",
              "       'whaler', 'whaley', 'wharfi', 'wharton', 'what', 'whatsoev',\n",
              "       'wheel', 'wheeli', 'whichev', 'whine', 'whip', 'whit', 'white',\n",
              "       'whiten', 'whitespac', 'wholesal', 'wholli', 'wi', 'wichita',\n",
              "       'wick', 'wide', 'wider', 'widespread', 'widget', 'width', 'wierd',\n",
              "       'wife', 'wil', 'wild', 'will', 'willi', 'william', 'wilson',\n",
              "       'wimsey', 'win', 'win3', 'win32', 'winbench', 'wind', 'window',\n",
              "       'windshield', 'wine', 'winfield', 'wing', 'wingat', 'winger',\n",
              "       'winmark', 'winner', 'winnipeg', 'winona', 'winter', 'winword',\n",
              "       'wip', 'wipe', 'wire', 'wirefram', 'wireless', 'wiretap', 'wisc',\n",
              "       'wisconsin', 'wisdom', 'wise', 'wish', 'wit', 'witch', 'withdraw',\n",
              "       'wittgenstein', 'wive', 'wizard', 'wk', 'wl', 'wlsmith', 'wm',\n",
              "       'wm4u', 'wmbxn', 'wn', 'wo', 'wod', 'wolf', 'wolv', 'wolverin',\n",
              "       'woman', 'women', 'won', 'wonder', 'wong', 'wont', 'wood',\n",
              "       'woodward', 'woof', 'worcest', 'word', 'wordperfect', 'work',\n",
              "       'workabl', 'workaround', 'worker', 'workgroup', 'workplac',\n",
              "       'workshop', 'workspac', 'workstat', 'world', 'worldwid', 'worm',\n",
              "       'worn', 'worri', 'wors', 'worship', 'worst', 'worth', 'worthi',\n",
              "       'worthless', 'worthwhil', 'wouldn', 'wound', 'wow', 'wp', 'wpd',\n",
              "       'wpg', 'wpi', 'wr', 'wrap', 'wrat', 'wray', 'wreck', 'wrench',\n",
              "       'wri', 'wright', 'wrigley', 'wrist', 'write', 'writer', 'written',\n",
              "       'wrong', 'wrote', 'ws', 'wsh', 'wsu', 'wt', 'wt7', 'wtm', 'wu',\n",
              "       'wuarchiv', 'wustl', 'wv', 'ww', 'ww2', 'ww7', 'wwc', 'wwhj',\n",
              "       'wwii', 'wwiz', 'x1', 'x11', 'x11r4', 'x11r5', 'x2', 'x3', 'x386',\n",
              "       'x5', 'x5w', 'x6', 'x7', 'x8', 'x86', 'x_g', 'x_s', 'x_scx', 'xa',\n",
              "       'xavier', 'xaw', 'xb', 'xc', 'xclrp', 'xcopyarea', 'xd',\n",
              "       'xdefault', 'xdm', 'xenon', 'xerox', 'xfree86', 'xga', 'xhost',\n",
              "       'xi', 'ximag', 'xj', 'xl', 'xlib', 'xloadimag', 'xm', 'xmosaic',\n",
              "       'xmu', 'xor', 'xp', 'xpert', 'xpm', 'xputimag', 'xr', 'xrdb', 'xs',\n",
              "       'xserver', 'xsizehint', 'xsun', 'xt', 'xte', 'xterm', 'xtpointer',\n",
              "       'xtsetarg', 'xuserfilesearchpath', 'xv', 'xview', 'xvoid', 'xw',\n",
              "       'xwd', 'xwindow', 'xx', 'xxdate', 'xxi', 'xxmessag', 'xxx', 'xxxx',\n",
              "       'xxxxx', 'xy', 'y0', 'y1', 'y2', 'y3', 'y4', 'y8', 'y_', 'ya',\n",
              "       'yalanc', 'yalcin', 'yale', 'yamaha', 'yamauchi', 'yang', 'yank',\n",
              "       'yanke', 'yard', 'yassin', 'yd', 'ye', 'yea', 'yeah', 'year',\n",
              "       'yeast', 'yee', 'yell', 'yellow', 'yeltsin', 'yep', 'yerevan',\n",
              "       'yes', 'yesterday', 'yf9', 'yfn', 'yg', 'yield', 'yigal', 'yj',\n",
              "       'yl', 'yn', 'yo', 'yob', 'yogi', 'york', 'yorku', 'young',\n",
              "       'younger', 'youngstown', 'yount', 'youth', 'yoyo', 'yr', 'ys',\n",
              "       'ysu', 'yt', 'yu', 'yugoslavia', 'yuma', 'yup', 'yuri', 'yx', 'yz',\n",
              "       'yzerman', 'z0', 'z1', 'z4', 'z5', 'z6e1t', 'z6ei', 'z7', 'za',\n",
              "       'zao', 'zaphod', 'zbh', 'zbhj', 'zbib', 'zc', 'zd', 'zd9',\n",
              "       'zealand', 'zen', 'zenith', 'zeo', 'zephyr', 'zero', 'zeus',\n",
              "       'zeus02', 'zf', 'zi', 'zinc', 'zionism', 'zionist', 'zip',\n",
              "       'zisfein', 'zl', 'zn', 'zone', 'zoo', 'zoolog', 'zoom',\n",
              "       'zoroastrian', 'zq', 'zs', 'zuma', 'zurich', 'zv', 'zx', 'zz'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance metrics"
      ],
      "metadata": {
        "id": "QzDkV02ETUqS"
      },
      "id": "QzDkV02ETUqS"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "detailed-disability",
      "metadata": {
        "id": "detailed-disability"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "traditional-engineering",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "traditional-engineering",
        "outputId": "2f98cd91-c702-4ef0-8b73-eddf6653245c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8138608603292619"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "accuracy_score(predicted_mnb_stemmed, twenty_test.target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "hispanic-myanmar",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hispanic-myanmar",
        "outputId": "18e3f9ff-7f21-48ac-b974-3152d2bac2f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8138608603292619"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "precision_score(predicted_mnb_stemmed, twenty_test.target, average='micro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "affecting-roulette",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "affecting-roulette",
        "outputId": "58112f04-d4b3-418d-9ec2-3ec4a0da630d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8226006672357936"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "recall_score(predicted_mnb_stemmed, twenty_test.target, average='macro')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification report"
      ],
      "metadata": {
        "id": "yP3GE0rrTbRs"
      },
      "id": "yP3GE0rrTbRs"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "retained-lightning",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "retained-lightning",
        "outputId": "97118580-456f-4f19-d2b5-438440fec554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.72      0.77      0.75       297\n",
            "           comp.graphics       0.75      0.69      0.72       423\n",
            " comp.os.ms-windows.misc       0.69      0.78      0.73       350\n",
            "comp.sys.ibm.pc.hardware       0.73      0.65      0.69       444\n",
            "   comp.sys.mac.hardware       0.83      0.80      0.82       397\n",
            "          comp.windows.x       0.79      0.83      0.81       378\n",
            "            misc.forsale       0.80      0.81      0.80       386\n",
            "               rec.autos       0.90      0.88      0.89       406\n",
            "         rec.motorcycles       0.93      0.93      0.93       401\n",
            "      rec.sport.baseball       0.92      0.92      0.92       396\n",
            "        rec.sport.hockey       0.98      0.92      0.95       428\n",
            "               sci.crypt       0.94      0.86      0.90       432\n",
            "         sci.electronics       0.65      0.79      0.72       323\n",
            "                 sci.med       0.79      0.91      0.85       344\n",
            "               sci.space       0.92      0.86      0.88       422\n",
            "  soc.religion.christian       0.93      0.69      0.79       540\n",
            "      talk.politics.guns       0.93      0.68      0.79       502\n",
            "   talk.politics.mideast       0.91      0.95      0.93       362\n",
            "      talk.politics.misc       0.55      0.86      0.67       198\n",
            "      talk.religion.misc       0.36      0.88      0.51       103\n",
            "\n",
            "                accuracy                           0.81      7532\n",
            "               macro avg       0.80      0.82      0.80      7532\n",
            "            weighted avg       0.83      0.81      0.82      7532\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(predicted_mnb_stemmed, twenty_test.target, target_names=twenty_train.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore the weights of the features in the trained SVM model in order to infer some insights related to feature importance for each class.\n"
      ],
      "metadata": {
        "id": "CQOoREW9Zn7x"
      },
      "id": "CQOoREW9Zn7x"
    },
    {
      "cell_type": "code",
      "source": [
        "# ...."
      ],
      "metadata": {
        "id": "6XvfOlDFbugY"
      },
      "id": "6XvfOlDFbugY",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = text_clf_svm['clf-svm']"
      ],
      "metadata": {
        "id": "1dJ0B7NcZv9w"
      },
      "id": "1dJ0B7NcZv9w",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.coef_.shape\n",
        "# <- investigate the features with highest coefficients\n",
        "print(classifier.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n82Wiv4Kabv-",
        "outputId": "79b62ad5-43d8-4c77-ddee-89d349d8eea4"
      },
      "id": "n82Wiv4Kabv-",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.05671016  0.04070699 -0.00475401 ...  0.         -0.00507972\n",
            "  -0.02184114]\n",
            " [-0.02280162 -0.0251327   0.         ... -0.00017215  0.10495375\n",
            "  -0.01687109]\n",
            " [-0.11747475  0.004907   -0.01854303 ...  0.00359518 -0.00324281\n",
            "   0.00083127]\n",
            " ...\n",
            " [-0.05260286  0.07714953 -0.00125346 ...  0.         -0.00282441\n",
            "   0.        ]\n",
            " [-0.02415987 -0.0005222   0.         ...  0.          0.\n",
            "  -0.00033254]\n",
            " [-0.05066426 -0.00853936  0.00340131 ...  0.          0.\n",
            "   0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(classifier.coef_[-1])[:10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOchjhdebsWs",
        "outputId": "ff7c9561-7dce-4be7-a795-6d6f95e00be8"
      },
      "id": "OOchjhdebsWs",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.15444660450574657,\n",
              " -0.13036697154635563,\n",
              " -0.12332716112994753,\n",
              " -0.11566964422150726,\n",
              " -0.11461752304091687,\n",
              " -0.11331696540274347,\n",
              " -0.11236593127407489,\n",
              " -0.10976382494350513,\n",
              " -0.10653733910275515,\n",
              " -0.10400696732522516]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises"
      ],
      "metadata": {
        "id": "VNXcG1uTDzlI"
      },
      "id": "VNXcG1uTDzlI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try different classification models from sklearn to classify the same data, for example a Logistic Regression model: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n"
      ],
      "metadata": {
        "id": "eEQZ2J4hD12X"
      },
      "id": "eEQZ2J4hD12X"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "zR2EsBtLD1J_"
      },
      "id": "zR2EsBtLD1J_",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you find a different preprocessing pipeline which leads to better results? For example, try the following:\n",
        "- lemmatization instead of stemming\n",
        "- include or exclude punctuation\n",
        "- limit the vocabulary size\n",
        "- include word n-grams in the vocabulary\n",
        "- ..."
      ],
      "metadata": {
        "id": "N1K1NnGocq3_"
      },
      "id": "N1K1NnGocq3_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore different features for the same dataset\n",
        "\n",
        "# Count Vectorizer\n",
        "count_vec_limited_vocab = CountVectorizer(max_features=2500)\n",
        "X_train_counts_limited_vocab = count_vec_limited_vocab.fit_transform(twenty_train.data)\n",
        "X_train_counts_limited_vocab.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhVPDusY4TrY",
        "outputId": "252b9935-87d8-4fb6-b443-ae1812c1ccb3"
      },
      "id": "rhVPDusY4TrY",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 2500)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_vec_limited_vocab.get_feature_names_out()[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4MWnBAe5MwJ",
        "outputId": "1873593b-42f6-4bff-f2ec-e17041f056c6"
      },
      "id": "z4MWnBAe5MwJ",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['00', '000', '01', '02', '03', '04', '05', '06', '07', '08', '09',\n",
              "       '0d', '0t', '10', '100', '1000', '11', '12', '128', '13', '130',\n",
              "       '14', '145', '15', '150', '16', '17', '18', '19', '1988', '1989',\n",
              "       '1990', '1991', '1992', '1993', '1993apr14', '1993apr15',\n",
              "       '1993apr16', '1993apr19', '1993apr20', '1993apr5', '1993apr6',\n",
              "       '1d9', '1st', '1t', '20', '200', '2000', '21', '22', '23', '24',\n",
              "       '241', '25', '250', '256', '26', '27', '28', '29', '2di', '2nd',\n",
              "       '2tm', '30', '300', '31', '32', '33', '34', '34u', '35', '36',\n",
              "       '37', '38', '386', '39', '3d', '3rd', '3t', '40', '400', '408',\n",
              "       '41', '42', '43', '44', '45', '46', '47', '48', '486', '49', '4t',\n",
              "       '50', '500', '51', '52', '53', '54', '55'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternatively: use TF-IDF features\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf_limited_vocab = tfidf_transformer.fit_transform(X_train_counts_limited_vocab)\n",
        "\n",
        "print(\"Number of features seen during fit: \", tfidf_transformer.n_features_in_)\n",
        "X_train_tfidf_limited_vocab.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIevzuDA5eAS",
        "outputId": "c77934b9-e768-4a98-d904-d9703a4923a5"
      },
      "id": "UIevzuDA5eAS",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features seen during fit:  2500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 2500)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIjd9QzVNpxU",
        "outputId": "b3ac360d-4a77-4292-f89e-ab35b2f737bd"
      },
      "id": "rIjd9QzVNpxU",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# any letter repeated more than three times in a row is replaced by two repetitions of the same letter\n",
        "def remove_multiple_occurences(text):\n",
        "    n = len(text)\n",
        "\n",
        "    if n < 3:\n",
        "        return text\n",
        "\n",
        "    i, count = 0, 0\n",
        "    while i < n - 1:\n",
        "        i += 1\n",
        "        if text[i] != text[i-1]:\n",
        "            count = 0\n",
        "        else:\n",
        "            count += 1\n",
        "            if count >= 2:\n",
        "                text = text[:i] + text[i+1:]\n",
        "                n -= 1\n",
        "                i -= 1\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "0X24sfEqX5bQ"
      },
      "id": "0X24sfEqX5bQ",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "import string\n",
        "import unicodedata\n",
        "\n",
        "wordnet_map = {\n",
        "    \"N\": wordnet.NOUN,\n",
        "    \"V\": wordnet.VERB,\n",
        "    \"J\": wordnet.ADJ,\n",
        "    \"R\": wordnet.ADV\n",
        "}\n",
        "\n",
        "class LemmatizedCountVectorizer(CountVectorizer):\n",
        "  def get_tokens(self, text):\n",
        "    # Normalize text\n",
        "    preprocessed_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    # Tokenize text\n",
        "    tokens = nltk.word_tokenize(preprocessed_text)\n",
        "    return tokens\n",
        "\n",
        "  def get_lemmas(self, text):\n",
        "    tokenized_text = self.get_tokens(text)\n",
        "    tagged_text = nltk.pos_tag(tokenized_text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = [lemmatizer.lemmatize(w, pos=wordnet_map.get(p[0], wordnet.NOUN)) for (w, p) in tagged_text]\n",
        "  \n",
        "    return lemmas\n",
        "\n",
        "  def build_analyzer(self):\n",
        "    analyzer = super(LemmatizedCountVectorizer, self).build_analyzer()\n",
        "    return lambda text: (self.get_lemmas(text))\n",
        "\n",
        "\n",
        "lemmatized_count_vec = LemmatizedCountVectorizer(stop_words=\"english\", max_features=7500)"
      ],
      "metadata": {
        "id": "VaLv1f7z8HGw"
      },
      "id": "VaLv1f7z8HGw",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  Logistic regression\n",
        "  Reference: https://scikit-learn.org/stable/modules/linear_model.html?highlight=logistic+regression#logistic-regression\n",
        "\"\"\"\n",
        "\n",
        "text_lr_lemmatized = Pipeline([\n",
        "    ('bag_of_words', lemmatized_count_vec),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('classifier', LogisticRegression(max_iter=120))\n",
        "])\n",
        "\n",
        "text_lr_lemmatized = text_lr_lemmatized.fit(twenty_train.data, twenty_train.target)\n",
        "predicted_lr_lemmatized = text_lr_lemmatized.predict(twenty_test.data)\n",
        "\n",
        "print('Accurary: ', np.mean(twenty_test.target == predicted_lr_lemmatized))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXzIvfv055Gv",
        "outputId": "4cf141ba-ba52-4b7a-a514-a31631b50b33"
      },
      "id": "uXzIvfv055Gv",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accurary:  0.7635422198619225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_lr_lemmatized['bag_of_words'].get_feature_names_out()[-100:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1mLtIQhV189",
        "outputId": "20f699a4-e72a-4bd4-96fa-3df62cb2a0b8"
      },
      "id": "w1mLtIQhV189",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['wide', 'widely', 'widespread', 'widget', 'width', 'wife', 'wild',\n",
              "       'will', 'willing', 'win', 'wind', 'window', 'windows', 'wing',\n",
              "       'winner', 'winter', 'wipe', 'wire', 'wiretap', 'wiring', 'wisdom',\n",
              "       'wise', 'wish', 'with', 'withdraw', 'within', 'without', 'witness',\n",
              "       'wlsmith', 'wo', 'woman', 'wonder', 'wonderful', 'wood', 'word',\n",
              "       'work', 'worker', 'working', 'workstation', 'world',\n",
              "       'world.std.com', 'worried', 'worry', 'worship', 'worth',\n",
              "       'worthless', 'worthwhile', 'worthy', 'would', \"wouldn't\", 'wound',\n",
              "       'wrap', 'wrist', 'write', 'writer', 'writes', 'writing', 'wrong',\n",
              "       'x', 'x-Soviet', 'xdm', 'xpert', 'xterm', 'y', 'ya', 'yard', 'ye',\n",
              "       'yeah', 'year', 'yeast', 'yell', 'yellow', 'yes', 'yesterday',\n",
              "       'yet', 'yfn.ysu.edu', 'yield', 'you', \"you're\", 'young', 'your',\n",
              "       'yours', 'yourself', 'youth', 'yoyo.cc.monash.edu.au', 'z', 'zero',\n",
              "       'zone', 'zoo.toronto.edu', 'zuma.UUCP', '{', '|', '|/', '|I', '|_',\n",
              "       '|__', '|_____|/', '||', '}', '~'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print(classification_report(predicted_lr_lemmatized, twenty_test.target, target_names=twenty_train.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urncPPnqWAxL",
        "outputId": "401f4329-9f41-4689-e7a8-b608103ba3e6"
      },
      "id": "urncPPnqWAxL",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.63      0.64      0.64       313\n",
            "           comp.graphics       0.70      0.61      0.66       446\n",
            " comp.os.ms-windows.misc       0.67      0.65      0.66       407\n",
            "comp.sys.ibm.pc.hardware       0.66      0.68      0.67       377\n",
            "   comp.sys.mac.hardware       0.71      0.79      0.75       347\n",
            "          comp.windows.x       0.75      0.77      0.76       388\n",
            "            misc.forsale       0.87      0.66      0.75       509\n",
            "               rec.autos       0.81      0.86      0.83       370\n",
            "         rec.motorcycles       0.87      0.87      0.87       395\n",
            "      rec.sport.baseball       0.84      0.84      0.84       396\n",
            "        rec.sport.hockey       0.91      0.91      0.91       398\n",
            "               sci.crypt       0.84      0.91      0.87       366\n",
            "         sci.electronics       0.70      0.65      0.68       424\n",
            "                 sci.med       0.74      0.83      0.78       355\n",
            "               sci.space       0.88      0.89      0.88       393\n",
            "  soc.religion.christian       0.90      0.76      0.83       474\n",
            "      talk.politics.guns       0.85      0.67      0.75       466\n",
            "   talk.politics.mideast       0.77      0.94      0.85       307\n",
            "      talk.politics.misc       0.55      0.64      0.59       265\n",
            "      talk.religion.misc       0.41      0.76      0.54       136\n",
            "\n",
            "                accuracy                           0.76      7532\n",
            "               macro avg       0.75      0.77      0.76      7532\n",
            "            weighted avg       0.77      0.76      0.76      7532\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use one of the sklearn models to train a classifier to predict the author of a text. You can use the `nltk` downloader to build a dataset: for example download literary texts written by Shakespeare and by Jane Austen, then build a binary classifier to distinguish between the two.\n",
        "\n",
        "Try to use feature selection to use different kinds of features than in the previous task:\n",
        "- stopword features\n",
        "- punctuation features\n",
        "- parts-of-speech (i.e. first apply a POS-tagger on the text, then use the obtained tags as your \"vocabulary\" instead of word tokens.)\n",
        "- use character-level representations (remember `analize`='char` for the sklearn vectorizers)"
      ],
      "metadata": {
        "id": "zuhfgQsmi33T"
      },
      "id": "zuhfgQsmi33T"
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_L_KEi5chGm",
        "outputId": "e75724f0-87b9-4272-f267-47abc18b7c31"
      },
      "id": "I_L_KEi5chGm",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.corpus.gutenberg.fileids()"
      ],
      "metadata": {
        "id": "E4hjDzxhjuPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06ada359-fcfc-4b15-c771-57d8140777eb"
      },
      "id": "E4hjDzxhjuPq",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth_sentences = nltk.corpus.gutenberg.sents('shakespeare-macbeth.txt')\n",
        "hamlet_sentences = nltk.corpus.gutenberg.sents('shakespeare-hamlet.txt')\n",
        "caesar_sentences = nltk.corpus.gutenberg.sents('shakespeare-caesar.txt')\n",
        "\n",
        "emma_sentences = nltk.corpus.gutenberg.sents('austen-emma.txt')\n",
        "sense_sentences = nltk.corpus.gutenberg.sents('austen-sense.txt')\n",
        "persuasion_sentences = nltk.corpus.gutenberg.sents('austen-persuasion.txt')"
      ],
      "metadata": {
        "id": "By-LW5bsorxv"
      },
      "id": "By-LW5bsorxv",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth_longest_len = max(len(s) for s in macbeth_sentences)\n",
        "hamlet_longest_len = max(len(s) for s in hamlet_sentences)\n",
        "caesar_longest_len = max(len(s) for s in caesar_sentences)\n",
        "\n",
        "emma_longest_len = max(len(s) for s in emma_sentences)\n",
        "sense_longest_len = max(len(s) for s in sense_sentences)\n",
        "persuasion_longest_len = max(len(s) for s in persuasion_sentences)"
      ],
      "metadata": {
        "id": "RAwUpBn9pXWw"
      },
      "id": "RAwUpBn9pXWw",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all the sentences that have at least half of the length of the longest sentence\n",
        "data_machbeth = [s for s in macbeth_sentences if len(s) > macbeth_longest_len / 4.]\n",
        "data_hamlet = [s for s in hamlet_sentences if len(s) > hamlet_longest_len / 4.]\n",
        "data_caesar = [s for s in caesar_sentences if len(s) > caesar_longest_len / 4.]\n",
        "\n",
        "data_emma = [s for s in emma_sentences if len(s) > emma_longest_len / 4.]\n",
        "data_sense = [s for s in sense_sentences if len(s) > sense_longest_len / 4.]\n",
        "data_persuasion = [s for s in persuasion_sentences if len(s) > persuasion_longest_len / 4.]"
      ],
      "metadata": {
        "id": "Q9m0pLgspwNq"
      },
      "id": "Q9m0pLgspwNq",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_sentences_shakespeare = len(data_hamlet) + len(data_caesar) + len(data_machbeth)\n",
        "num_sentences_austen = len(data_emma) + len(data_sense) + len(data_persuasion)"
      ],
      "metadata": {
        "id": "W-GfqOI0rysC"
      },
      "id": "W-GfqOI0rysC",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.concatenate([data_machbeth, data_hamlet, data_caesar, data_emma, data_sense, data_persuasion])\n",
        "y = np.concatenate([np.zeros((num_sentences_shakespeare),), np.ones((num_sentences_austen,))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwDW7pcqqZZW",
        "outputId": "c7dd88d7-9d17-4591-dd99-cc5ab47988d5"
      },
      "id": "jwDW7pcqqZZW",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data intro train and test\n",
        "X_train_texts, X_test_texts, y_train_texts, y_test_texts = train_test_split(X, y, test_size=0.33, random_state=0)"
      ],
      "metadata": {
        "id": "UOUE4WM7rAZ6"
      },
      "id": "UOUE4WM7rAZ6",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PosTagsCountVectorizer(CountVectorizer):\n",
        "  def get_pos_tags(self, tokenized_text):\n",
        "    tagged_text = nltk.pos_tag(tokenized_text)\n",
        "    pos_tags = [p for (_, p) in tagged_text]  \n",
        "    return pos_tags\n",
        "\n",
        "  def build_analyzer(self):\n",
        "    analyzer = super(PosTagsCountVectorizer, self).build_analyzer()\n",
        "    return lambda text: (self.get_pos_tags(text))\n",
        "\n",
        "\n",
        "postags_count_vec = PosTagsCountVectorizer(stop_words=\"english\", max_features=2500, analyzer='char')"
      ],
      "metadata": {
        "id": "u1qYoYemsaZI"
      },
      "id": "u1qYoYemsaZI",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  Logistic regression\n",
        "  Reference: https://scikit-learn.org/stable/modules/linear_model.html?highlight=logistic+regression#logistic-regression\n",
        "\"\"\"\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "text_rf_postags = Pipeline([\n",
        "    ('bag_of_words', postags_count_vec),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('classifier', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "text_rf_lemmatized = text_rf_postags.fit(X_train_texts, y_train_texts)\n",
        "predicted_rf_postags = text_rf_postags.predict(X_test_texts)"
      ],
      "metadata": {
        "id": "SsSaN4yctALs"
      },
      "id": "SsSaN4yctALs",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accurary: ', np.mean(predicted_rf_postags == y_test_texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hd-Kc1cPtoZv",
        "outputId": "2ef83914-0137-49a8-bac3-1f3a3a5af102"
      },
      "id": "hd-Kc1cPtoZv",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accurary:  0.9484126984126984\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}