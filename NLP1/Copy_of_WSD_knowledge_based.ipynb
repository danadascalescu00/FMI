{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of WSD_knowledge-based.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# WordNet- based algorithms"
      ],
      "metadata": {
        "id": "DXrzlO_EpETf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lesk's algorithm"
      ],
      "metadata": {
        "id": "K6FxKQdFpNXI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FHkkZMJIlqr",
        "outputId": "21dc58ae-04cf-49a3-d257-3f9e928706d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.wsd import lesk\n",
        "from nltk.corpus import wordnet as wn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nV56U32fZoE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lesk(nltk.word_tokenize('Today I went to the bank to ask for a loan.'),'bank','n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siq0Zfo-tkGd",
        "outputId": "a1cd55f9-5367-4f37-81ca-f6a517715d63"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('savings_bank.n.02')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lesk??"
      ],
      "metadata": {
        "id": "Ou-zB4teysJJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[s.definition() for s in wn.synsets('loan')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWDLZ-P382_5",
        "outputId": "29b22704-13aa-4e8c-9db6-cac16ab991bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the temporary provision of money (usually at interest)',\n",
              " \"a word borrowed from another language; e.g. `blitz' is a German word borrowed into modern English\",\n",
              " 'give temporarily; let have for a limited time']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syns=wn.synsets('bank', 'n')\n",
        "for s in syns:\n",
        "  print(s, s.definition())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdptK958twX4",
        "outputId": "9e186d21-833c-4c3a-b295-d36c553ff907"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('bank.n.01') sloping land (especially the slope beside a body of water)\n",
            "Synset('depository_financial_institution.n.01') a financial institution that accepts deposits and channels the money into lending activities\n",
            "Synset('bank.n.03') a long ridge or pile\n",
            "Synset('bank.n.04') an arrangement of similar objects in a row or in tiers\n",
            "Synset('bank.n.05') a supply or stock held in reserve for future use (especially in emergencies)\n",
            "Synset('bank.n.06') the funds held by a gambling house or the dealer in some gambling games\n",
            "Synset('bank.n.07') a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
            "Synset('savings_bank.n.02') a container (usually with a slot in the top) for keeping money at home\n",
            "Synset('bank.n.09') a building in which the business of banking transacted\n",
            "Synset('bank.n.10') a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synsets('savings_bank')[1].hyponyms()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKQau7qk9nnK",
        "outputId": "a3c8755d-8d3c-40c2-fb82-8bbf5053b1ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('piggy_bank.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lesk(nltk.word_tokenize('Students enjoy going to school, studying and reading books'),'school','n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kio2RbIYI-Lx",
        "outputId": "c8e74eef-b077-441e-a52b-cb1cc9cd20ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('school.n.06')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syns = wn.synsets('school', 'n')\n",
        "for s in syns:\n",
        "  print(s, s.definition())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PR55MIDtpo8o",
        "outputId": "6158f117-ea4f-4683-ffea-ebe107e1853e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('school.n.01') an educational institution\n",
            "Synset('school.n.02') a building where young people receive education\n",
            "Synset('school.n.03') the process of being formally educated at a school\n",
            "Synset('school.n.04') a body of creative artists or writers or thinkers linked by a similar style or by similar teachers\n",
            "Synset('school.n.05') the period of instruction in a school; the time period when school is in session\n",
            "Synset('school.n.06') an educational institution's faculty and students\n",
            "Synset('school.n.07') a large group of fish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synsets('school')[5].examples()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2itIx9u-atWG",
        "outputId": "9189b495-7e3d-48c6-b071-ad1781939051"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the school keeps parents informed',\n",
              " 'the whole school turned out for the game']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lesk(nltk.word_tokenize('I was snorkeling and saw a school of trout'),'school','n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrHfUXka-qEq",
        "outputId": "846c5da9-43df-41e3-86e8-167ced5f9bdf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('school.n.05')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synsets('trout')[1].definition()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FuvepsEKb0Hb",
        "outputId": "c45cd197-f7b7-4ae3-c867-1206943c85df"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'any of various game and food fishes of cool fresh waters mostly smaller than typical salmons'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = lesk(nltk.word_tokenize(\"I went fishing for some sea bass\"), 'bass', 'n')\n",
        "print(s, s.definition())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnZFqszhI268",
        "outputId": "eb138156-c5d8-491a-fa1d-aac26ab8d0c3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('bass.n.08') nontechnical name for any of numerous edible marine and freshwater spiny-finned fishes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for s in wn.synsets('bass'):\n",
        "  print(s, s.definition())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u2mxgBUJlqJ",
        "outputId": "6ac1131a-e70d-44ea-d799-8ecf3919f55c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('bass.n.01') the lowest part of the musical range\n",
            "Synset('bass.n.02') the lowest part in polyphonic music\n",
            "Synset('bass.n.03') an adult male singer with the lowest voice\n",
            "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n",
            "Synset('freshwater_bass.n.01') any of various North American freshwater fish with lean flesh (especially of the genus Micropterus)\n",
            "Synset('bass.n.06') the lowest adult male singing voice\n",
            "Synset('bass.n.07') the member with the lowest range of a family of musical instruments\n",
            "Synset('bass.n.08') nontechnical name for any of numerous edible marine and freshwater spiny-finned fishes\n",
            "Synset('bass.s.01') having or denoting a low vocal or instrumental range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syns=wn.synsets('trout', 'n')\n",
        "for s in syns:\n",
        "  print(s, s.member_meronyms())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHpaFpMmhO9P",
        "outputId": "ec4faaba-39e2-449d-8f41-6b2fbed4507b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('trout.n.01') []\n",
            "Synset('trout.n.02') []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extension: Banerjee and Pedersen's algorithm"
      ],
      "metadata": {
        "id": "FwuyMy2nzOYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This technique was presented by Satanjeev Banerjee and Ted Pedersen in 2003 in an article. https://www.researchgate.net/publication/221629283_An_Adapted_Lesk_Algorithm_for_Word_Sense_Disambiguation_Using_WordNet\n",
        "\n",
        "The algorithm measures the relatedness of two words. Just like Lesk, it counts the overlaps of glosses, however it takes into account the related glosses of the two words as well.\n",
        "\n",
        "Suppose we want to obtain the sense for a word in a certain context (for example a sentence or just a window of text). The steps of the algorithm are:\n",
        "\n",
        "We first tag the words in the sentence with their part of speech\n",
        "For each word we obtain the list of synsets corresponding to that part of speech.\n",
        "\n",
        "For each synset s we obtain the glosses of the synsets for all:\n",
        "\n",
        "- hypernyms\n",
        "- hyponyms\n",
        "- meronyms\n",
        "- holonyms\n",
        "- troponyms\n",
        "- attributes\n",
        "- similar–to\n",
        "- also–see\n",
        "\n",
        "it is good to use a structure that shows for each gloss where it comes from (in order to do the tests in the exercise). We add them all in a list with all the glosses (for each target word). We call these lists \"**extended glosses**\".\n",
        "\n",
        "For each synnset of the target word (for which we want to obtain the sense) we compute a score by counting the overlaps in the synset with all the other synsets corresponding to the words in the context.\n",
        "In computing the score, for each single word that appears in both extended glosses we add 1. However if it appears in a common phrase, supposing the length of common phrase is L, we add L**2 (for example, if \"white bread\" appears in both glosses, we add 4). We obviously don't add the score for the separate words in the phrase. \n",
        "\n",
        "We try to find the longest common sequences of consecutive words (it shouldn't start or end with a pronoun, preposition, article or conjunction in both glosses). In order to avoid counting the same overlap multiple times for the same two glosses, after counting the overlap you should replace the sequence of words with a special string (don't remove it completely as you may obtain false overlaps)\n",
        "After computing the score for each synset of the target word, choose as result the synset with the highest score."
      ],
      "metadata": {
        "id": "mPcehk1oNw4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"some white snow and some type of rye bread\" ## 2\n",
        "\"more white bread with butter\"\n",
        "\n",
        "\"i eat white bread for breakfast\" ## 2?\n",
        "\"white bread is better than rye\"\n",
        "\n",
        "\"bread and butter\""
      ],
      "metadata": {
        "id": "1seuopbXdGdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ba43bad2-d87b-447e-bf90-7494fc6fb923"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bread and butter'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "\n",
        "gloss_rel = lambda x: x.definition()\n",
        "example_rel = lambda x: \" \".join(x.examples())\n",
        "hyponym_rel = lambda x: \" \".join(w.definition() for w in x.hyponyms())\n",
        "meronym_rel = lambda x: \" \".join(w.definition() for w in x.member_meronyms() + \\\n",
        "                                 x.part_meronyms() + x.substance_meronyms())\n",
        "also_rel = lambda x: \" \".join(w.definition() for w in x.also_sees())\n",
        "attr_rel = lambda x: \" \".join(w.definition() for w in x.attributes())\n",
        "hypernym_rel = lambda x: \" \".join(w.definition() for w in x.hypernyms())\n",
        "\n",
        "relpairs = {wn.NOUN: [(hyponym_rel, meronym_rel), (meronym_rel, hyponym_rel),\n",
        "                      (hyponym_rel, hyponym_rel),\n",
        "                      (gloss_rel, meronym_rel), (meronym_rel, gloss_rel),\n",
        "                      (example_rel, meronym_rel), (meronym_rel, example_rel),\n",
        "                      (gloss_rel, gloss_rel)],\n",
        "            wn.ADJ: [(also_rel, gloss_rel), (gloss_rel, also_rel),\n",
        "                     (attr_rel, gloss_rel), (gloss_rel, attr_rel),\n",
        "                     (gloss_rel, gloss_rel),\n",
        "                     (example_rel, gloss_rel), (gloss_rel, example_rel),\n",
        "                     (gloss_rel, hypernym_rel), (hypernym_rel, gloss_rel)],\n",
        "            wn.VERB:[(example_rel, example_rel),\n",
        "                     (example_rel, hypernym_rel), (hypernym_rel, example_rel),\n",
        "                     (hyponym_rel, hyponym_rel),\n",
        "                     (gloss_rel, hyponym_rel), (hyponym_rel, gloss_rel),\n",
        "                     (example_rel, gloss_rel), (gloss_rel, example_rel)]}\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    Helper function to preprocess text (lowercase, remove punctuation etc.)\n",
        "    \"\"\"\n",
        "    words = nltk.word_tokenize(text)\n",
        "    punctuation = string.punctuation\n",
        "    words = [word.lower() for word in words if word not in punctuation]\n",
        "    words = [word for word in words if not word in stopwords.words('english')] # ? not part of the original algorithm to remove all stopwords! (only ones at the edges of the subsequence)\n",
        "    return words\n",
        "\n",
        "def lcs(S1, S2):\n",
        "    \"\"\"\n",
        "    Helper function to compute length and offsets of longest common substring of\n",
        "    S1 and S2. Uses the classical dynamic programming algorithm.\n",
        "    \"\"\"\n",
        "    M = [[0]*(1+len(S2)) for i in range(1+len(S1))]\n",
        "    longest, x_longest, y_longest = 0, 0, 0\n",
        "    for x in range(1,1+len(S1)):\n",
        "        for y in range(1,1+len(S2)):\n",
        "            if S1[x-1] == S2[y-1]:\n",
        "                M[x][y] = M[x-1][y-1] + 1\n",
        "                if M[x][y]>longest:\n",
        "                    longest = M[x][y]\n",
        "                    x_longest = x\n",
        "                    y_longest = y\n",
        "            else:\n",
        "                M[x][y] = 0\n",
        "    return longest, x_longest - longest, y_longest - longest\n",
        "\n",
        "def score(gloss1, gloss2, normalized=False):\n",
        "    \"\"\"\n",
        "    Compute score between two glosses based on length of common substrings.\n",
        "    \"\"\"\n",
        "    gloss1 = preprocess(gloss1)\n",
        "    gloss2 = preprocess(gloss2)\n",
        "    curr_score = 0\n",
        "    longest, start1, start2, = lcs(gloss1, gloss2)\n",
        "    while longest > 0:\n",
        "        gloss1[start1 : start1 + longest] = []\n",
        "        gloss2[start2 : start2 + longest] = []\n",
        "        curr_score += longest ** 2\n",
        "        longest, start1, start2 = lcs(gloss1, gloss2)\n",
        "    if normalized and curr_score:\n",
        "      return curr_score / (len(gloss1) + len(gloss2))\n",
        "    return curr_score\n",
        "\n",
        "def relatedness(sense1, sense2, relpairs, normalized=False):\n",
        "    \"\"\"\n",
        "    Compute the relatedness of two senses (synsets) using the list of pairs of\n",
        "    relations in relpairs.\n",
        "    \"\"\"\n",
        "    return sum(score(pair[0](sense1), pair[1](sense2), normalized=normalized) # Note: normalization not explicitly part of original algorithm!\n",
        "    for pair in relpairs)\n",
        "\n",
        "def wsd(context, target, winsize, pos_tag, verbose=False, normalized=False):\n",
        "    \"\"\"\n",
        "    Find the best sense for a word in a given context.\n",
        "    Arguments:\n",
        "    context - sentence(s) we are analyzing; expected as list of strings\n",
        "    target  - string representing the word whose senses we're trying to\n",
        "              disambiguate. Target is assumed to occur once in sentence. In case\n",
        "              of multiple occurences, the first one is considered. Will throw\n",
        "              ValueError if target is not in sentence\n",
        "    winsize - size of window used for disambiguating. The algorithm will only\n",
        "              look at winsize words of the appropriate part-of-speech around the\n",
        "              target word\n",
        "    pos_tag - part of speech of target word\n",
        "    \"\"\"\n",
        "    context = list(filter(None, [wn.synsets(word, pos=pos_tag) for word in context]))\n",
        "    target_synsets = wn.synsets(target, pos=pos_tag)\n",
        "    pos = context.index(target_synsets)\n",
        "    window = context[max(pos - winsize, 0) : pos] + \\\n",
        "             context[pos + 1 : min(pos + winsize + 1, len(context))]\n",
        "    sense_scores = [sum(sum(relatedness(sense, other_sense, relpairs[pos_tag], normalized=normalized)\n",
        "                              for other_sense in senses)\n",
        "                   for senses in window) for sense in target_synsets]\n",
        "    if verbose:\n",
        "      print(\"All scores:\")\n",
        "      for i, s in enumerate(target_synsets):\n",
        "        print(sense_scores[i], s, s.definition())\n",
        "    best_score = max(sense_scores)\n",
        "    best_index = sense_scores.index(best_score)\n",
        "    return target_synsets[best_index], best_score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSZo5ZWcFUjy",
        "outputId": "56c95af2-c018-43f8-95ca-b024466fc156"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentence = \"I went fishing for some sea bass\"\n",
        "sentence = preprocess(sentence)\n",
        "sense, outscore = wsd(sentence, \"bass\", 5, wn.NOUN)\n",
        "print(\"best score was {} by synset {} \\n(definition = '{}')\".format(outscore, sense, sense.definition()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdCquF9PFi7I",
        "outputId": "362521eb-3f50-4884-c5e3-76235604a5c4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best score was 1 by synset Synset('sea_bass.n.01') \n",
            "(definition = 'the lean flesh of a saltwater fish of the family Serranidae')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Students enjoy going to school, studying and reading books\"\n",
        "sentence = preprocess(sentence)\n",
        "sense, outscore = wsd(sentence, \"school\", 1, wn.NOUN)\n",
        "print(\"best score was {} by synset {} \\n(definition = '{}')\".format(outscore, sense, sense.definition()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWMZLAe0JYYj",
        "outputId": "b18bd994-0a9e-411d-8462-9cdb500bc12b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best score was 1 by synset Synset('school.n.01') \n",
            "(definition = 'an educational institution')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I went to the bank to get a loan\"\n",
        "sentence = preprocess(sentence)\n",
        "sense, outscore = wsd(sentence, \"bank\", 3, wn.NOUN)\n",
        "print(\"best score was {} by synset {} \\n(definition = '{}')\".format(outscore, sense, sense.definition()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9UwHpf9J16e",
        "outputId": "1d082fe7-a94e-449d-f435-46c936e06bb0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best score was 12 by synset Synset('depository_financial_institution.n.01') \n",
            "(definition = 'a financial institution that accepts deposits and channels the money into lending activities')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I went to the bank to get a loan\"\n",
        "sentence = preprocess(sentence)\n",
        "sense, outscore = wsd(sentence, \"bank\", 2, wn.NOUN)\n",
        "print(\"best score was {} by synset {} \\n(definition = '{}')\".format(outscore, sense, sense.definition()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPaPoiJ1DA_o",
        "outputId": "90287ba4-87da-42f0-e791-2d83ce1f0f95"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best score was 12 by synset Synset('depository_financial_institution.n.01') \n",
            "(definition = 'a financial institution that accepts deposits and channels the money into lending activities')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'I was snorkeling and saw a school of trout'\n",
        "sentence = preprocess(sentence)\n",
        "sense, outscore = wsd(sentence, \"school\", 3, wn.NOUN, verbose=True, normalized=True)\n",
        "print(\"best score was {} by synset {} \\n(definition = '{}')\".format(outscore, sense, sense.definition()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybEnNVD4DM6p",
        "outputId": "d0bc29b4-07ce-4d02-8fbf-508287d0b684"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All scores:\n",
            "0.030312006319115323 Synset('school.n.01') an educational institution\n",
            "0.07142857142857142 Synset('school.n.02') a building where young people receive education\n",
            "0 Synset('school.n.03') the process of being formally educated at a school\n",
            "0.062405820142393036 Synset('school.n.04') a body of creative artists or writers or thinkers linked by a similar style or by similar teachers\n",
            "0.018867924528301886 Synset('school.n.05') the period of instruction in a school; the time period when school is in session\n",
            "0 Synset('school.n.06') an educational institution's faculty and students\n",
            "0.125 Synset('school.n.07') a large group of fish\n",
            "best score was 0.125 by synset Synset('school.n.07') \n",
            "(definition = 'a large group of fish')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[s.definition() for s in wn.synsets('whale')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW1S65jaHPhl",
        "outputId": "ce2e6f5e-5382-4f23-c92d-0ada6f39d75a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a very large person; impressive in size or qualities',\n",
              " 'any of the larger cetacean mammals having a streamlined body and breathing through a blowhole on the head',\n",
              " 'hunt for whales']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge-rich WSD based on WordNet++\n"
      ],
      "metadata": {
        "id": "fLjUtpI83Pp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technique developped by Simone Paolo Ponzetto and Roberto Navigli in their article \"Knowledge-rich Word Sense Disambiguation Rivaling Supervised Systems\". This approach uses supplimentary relations between words, in order to compute relatedness between concepts. https://aclanthology.org/P10-1154/\n",
        "\n",
        "All the new relations are based on Wikipedia, this is the reason why in this laboratory we need to use the Wikipedia module (documentation: https://wikipedia.readthedocs.io/en/latest/code.html). However, some of the needed relations are not implemented in the Wikipedia module, therefore, we will also need to use the requests module in order to use MediaWiki action API to wich we'll need to transmit requests.\n",
        "\n",
        "**Types of relations**\n",
        "\n",
        "- \"Redirect to\" relations (https://www.mediawiki.org/wiki/API:Redirects)\n",
        "- disambiguation pages\n",
        "- internal links\n",
        "\n",
        "In order to use these relations we need a mapping between WordNet word senses and Wikipedia articles. In the article, they give as an example the word \"soda\" (https://en.wikipedia.org/wiki/Soda). Notice that the disambiguation page redirects to this same page: https://en.wikipedia.org/wiki/Soda_(disambiguation). You can see that it has multiple senses illustrated in a list of pages. you can obtain the ids of those pages with a code similar to :"
      ],
      "metadata": {
        "id": "Et5GKe5CL95E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "#create a connection(session)\n",
        "r_session = requests.Session()\n",
        "\n",
        "#url for the MediaWiki action API\n",
        "URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "PARAMS = {\n",
        "    \"action\": \"query\", #we are creating a query\n",
        "    \"titles\": \"car\", #for the title car    \n",
        "    \"prop\": \"redirects\", #asking for all the redirects (to the title car)\n",
        "    \"format\": \"json\" #and we want the output in a json format\n",
        "}\n",
        "\n",
        "#we obtain the response to the get request with the given parmeters\n",
        "query_response = r_session.get(url=URL, params=PARAMS)\n",
        "json_data = query_response.json()\n",
        "\n",
        "wikipedia_pages = json_data[\"query\"][\"pages\"]\n",
        "\n",
        "#we iterate through items and print all the redirects (their title and id)\n",
        "try:\n",
        "    for k, v in wikipedia_pages.items():\n",
        "        for redir in v[\"redirects\"]:\n",
        "            print(\"{} redirect to {}({})\".format(redir[\"title\"], v[\"title\"], redir[\"pageid\"]))\n",
        "except KeyError as err:\n",
        "    if err.args[0]=='redirects':\n",
        "        print(\"It has no redirects\")\n",
        "    else:\n",
        "        print(repr(err))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL4ac809rXfQ",
        "outputId": "b4db6af1-1903-4224-f28d-9fbbc90859db"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cars redirect to Car(73688)\n",
            "Motor car redirect to Car(458458)\n",
            "Motorcar redirect to Car(458459)\n",
            "Automobiles redirect to Car(513608)\n",
            "Motor Car redirect to Car(840650)\n",
            "Passenger cars redirect to Car(1288645)\n",
            "Ottomobile redirect to Car(1836567)\n",
            "Automobles redirect to Car(1842410)\n",
            "Motorization redirect to Car(3223435)\n",
            "Motorisation redirect to Car(3223436)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "JSON data looks like this:"
      ],
      "metadata": {
        "id": "PZTiErE0REBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "   \"continue\":{\n",
        "      \"rdcontinue\":\"6492781\",\n",
        "      \"continue\":\"||\"\n",
        "   },\n",
        "   \"query\":{\n",
        "      \"normalized\":[\n",
        "         {\n",
        "            \"from\":\"car\",\n",
        "            \"to\":\"Car\"\n",
        "         }\n",
        "      ],\n",
        "      \"pages\":{\n",
        "         \"13673345\":{\n",
        "            \"pageid\":13673345,\n",
        "            \"ns\":0,\n",
        "            \"title\":\"Car\",\n",
        "            \"redirects\":[\n",
        "               {\n",
        "                  \"pageid\":73688,\n",
        "                  \"ns\":0,\n",
        "                  \"title\":\"Cars\"\n",
        "               },\n",
        "               {\n",
        "                  \"pageid\":458458,\n",
        "                  \"ns\":0,\n",
        "                  \"title\":\"Motor car\"\n",
        "               },\n",
        "               {\n",
        "                  \"pageid\":458459,\n",
        "                  \"ns\":0,\n",
        "                  \"title\":\"Motorcar\"\n",
        "               },\n",
        "               {\n",
        "                  \"pageid\":513608,\n",
        "                  \"ns\":0,\n",
        "                  \"title\":\"Automobiles\"\n",
        "               },\n",
        "               {\n",
        "                  \"pageid\":840650,\n",
        "                  \"ns\":0,\n",
        "                  \"title\":\"Motor Car\"\n",
        "               },\n",
        "               {\n",
        "                  \"pageid\":1836567,\n",
        "                  \"ns\":0,\n",
        "                  \"title\":\"Ottomobile\"\n",
        "               },\n",
        "               {\n",
        "                  \"pageid\":1842410,\n",
        "                  \"ns\":0,\n",
        "                  \"title\":\"Automobles\"\n",
        "               },\n",
        "               {\n",
        "                  \"pageid\":3223435,\n",
        "                  \"ns\":0,\n",
        "                  \"title\":\"Motorization\"\n",
        "               },\n",
        "               {\n",
        "                  \"pageid\":3223436,\n",
        "                  \"ns\":0,\n",
        "                  \"title\":\"Motorisation\"\n",
        "               },\n",
        "               {\n",
        "                  \"pageid\":6260924,\n",
        "                  \"ns\":0,\n",
        "                  \"title\":\"Passenger Vehicle\"\n",
        "               }\n",
        "            ]\n",
        "         }\n",
        "      }\n",
        "   }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PS7-DMvrYyG",
        "outputId": "64482e8f-85e3-42b4-db87-1acea321d3cc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'continue': {'continue': '||', 'rdcontinue': '6492781'},\n",
              " 'query': {'normalized': [{'from': 'car', 'to': 'Car'}],\n",
              "  'pages': {'13673345': {'ns': 0,\n",
              "    'pageid': 13673345,\n",
              "    'redirects': [{'ns': 0, 'pageid': 73688, 'title': 'Cars'},\n",
              "     {'ns': 0, 'pageid': 458458, 'title': 'Motor car'},\n",
              "     {'ns': 0, 'pageid': 458459, 'title': 'Motorcar'},\n",
              "     {'ns': 0, 'pageid': 513608, 'title': 'Automobiles'},\n",
              "     {'ns': 0, 'pageid': 840650, 'title': 'Motor Car'},\n",
              "     {'ns': 0, 'pageid': 1836567, 'title': 'Ottomobile'},\n",
              "     {'ns': 0, 'pageid': 1842410, 'title': 'Automobles'},\n",
              "     {'ns': 0, 'pageid': 3223435, 'title': 'Motorization'},\n",
              "     {'ns': 0, 'pageid': 3223436, 'title': 'Motorisation'},\n",
              "     {'ns': 0, 'pageid': 6260924, 'title': 'Passenger Vehicle'}],\n",
              "    'title': 'Car'}}}}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the normalization field, it is not what you might expect; it doesn't obtain the lemma, or apply any transformation on the letter case, it is about Unicode normalization.\n",
        "\n",
        "for disambiaguations, notice the following two links:\n",
        "\n",
        "https://en.wikipedia.org/w/api.php?action=query&titles=soda&prop=pageprops&format=json\n",
        "https://en.wikipedia.org/w/api.php?action=query&titles=car&prop=pageprops&format=json\n",
        "\n",
        "In order to create the mapping we shall use for a given Wikipedia page's:\n",
        "\n",
        "- sense labels (actually they are the titles of the pages. At the time when the article was written, the titles had this syntax \"word(sense label)\" like \"soda(soft drink)\", however, notice that now you only find the sense label as a title.\n",
        "- links (outgoing links from the current page)\n",
        "- categories\n",
        "\n",
        "The article uses the notation Ctx(w) for the set of words obtained from the text of some or all these pages.\n",
        "\n",
        "Next, we need the WordNet context for a sense s, Ctx(s), for each sense of the word. For this we take the following relations:\n",
        "\n",
        "- synonymy\n",
        "- hypernymy/hyponymy\n",
        "- sisterhood (senses that have the same direct hypernym)\n",
        "- gloss\n",
        "\n",
        "The next step is the mapping\n",
        "\n",
        "1. For each word that we want to disambiguate, if we have **only one sense**, and only one Wikipedia page, we map that Wikipedia page to the word.\n",
        "2. In the case of multiple senses, for each remaining wikipedia word w (after the mapping from the former step) that still has no associate Wordnet word, we take all the redirects to the word w. For each such redirect we look if we already have a mapping associated to it (a relation between its sense and the wikipedia page). If we have such a mapping and the mapped word is in w's sysnset, we map w to the sense associated to the redirect page\n",
        "3. For all wikipedia pages that aren't mapped yet, we try to assign the most probable sense. The most probable sense has the highest value p computed as score(w,sense)/sum, where sum is the sum between all the combinations of scores between each sense of the word from wordnet and each sense of the word from wikipedia. The score is the number of common words between the context of the sense in wikipedia and the context of the sense in WordNet to which we add 1: $score(s,w)=|Ctx(s) ∩ Ctx(w)|+1$\n",
        "\n",
        "In the end we have created new relations (WordNet++) that we can use in a **simplified Lesk manner** to disambiguate a text. We will compute the overlaps on all the glosses given by the mentioned relations.\n",
        "\n"
      ],
      "metadata": {
        "id": "NxdrU6FrPyIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I was drinking a soda with a straw.\""
      ],
      "metadata": {
        "id": "0ETKoO1dL5Xr"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}