{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbGSaGQ9vIS3"
      },
      "source": [
        "# NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07IDg6-UvIS6"
      },
      "source": [
        "#### Install NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGzlCrIivIS7",
        "outputId": "64bdbcb6-ca24-413e-aa81-c59f13289f34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B58lHrhEvIS9"
      },
      "source": [
        "#### Download models or corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwQRlgElvIS9",
        "outputId": "cac88fd8-9663-4ceb-da27-6ec65af9d160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "bash: line 1: import: command not found\n",
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/nltk/downloader.py\", line 982, in _interactive_download\n",
            "    DownloaderGUI(self).mainloop()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/nltk/downloader.py\", line 1226, in __init__\n",
            "    top = self.top = Tk()\n",
            "  File \"/usr/lib/python3.7/tkinter/__init__.py\", line 2023, in __init__\n",
            "    self.tk = _tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use)\n",
            "_tkinter.TclError: no display name and no $DISPLAY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/nltk/downloader.py\", line 2278, in <module>\n",
            "    halt_on_error=options.halt_on_error)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/nltk/downloader.py\", line 661, in download\n",
            "    self._interactive_download()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/nltk/downloader.py\", line 984, in _interactive_download\n",
            "    DownloaderShell(self).run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/nltk/downloader.py\", line 1006, in run\n",
            "    user_input = input('Downloader> ').strip()\n",
            "EOFError: EOF when reading a line\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "import nltk\n",
        "python -m nltk.downloader # shows a window when graphical output available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qvj0wdJmvIS-",
        "outputId": "e4faf22d-5f27-4de2-f964-a573b337b3a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQEsyB5JvITB"
      },
      "source": [
        "#### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dN3HUcBEvIS_"
      },
      "outputs": [],
      "source": [
        "tweet = \"RT @lOR42wsOEFcv3f: I fall too fast, crash too hard, forgive too easily and care too much... :( #amiright\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2kl8O_CcvITA"
      },
      "outputs": [],
      "source": [
        "query = 'fast'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFK_-15yvIS9"
      },
      "source": [
        "The naive way..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz3v_jj0vITA",
        "outputId": "89c19d28-27f8-49d0-c53b-fd26791e1603"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "tweet.find(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xhFpKFpvITB",
        "outputId": "b6a00aad-5c26-44b0-9a20-222e8facc94b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " '@lOR42wsOEFcv3f:',\n",
              " 'I',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast,',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard,',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much...',\n",
              " ':(',\n",
              " '#amiright']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tweet.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sx9xgcFvITC",
        "outputId": "a56a1312-b55d-4e28-d6e3-17815e47db56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[False]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "[query in tweet.split()]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correct tokenization: informed splitting of the text into tokens"
      ],
      "metadata": {
        "id": "gjhBVUZK1LV4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoieLiJcvITC",
        "outputId": "45e1e434-8f61-4535-9bb1-1638dec0734a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " '@',\n",
              " 'lOR42wsOEFcv3f',\n",
              " ':',\n",
              " 'I',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " ',',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " ',',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':',\n",
              " '(',\n",
              " '#',\n",
              " 'amiright']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "nltk.word_tokenize(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IA6xDrMdvITC",
        "outputId": "882e8539-8e0d-4353-bbba-9b6c3173d4e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[True]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "[query in nltk.word_tokenize(tweet)]\n",
        "# query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA5qgGixvITD",
        "outputId": "b5921531-6a92-4f12-ddec-9da73ff1db6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " '@',\n",
              " 'lOR42wsOEFcv3f',\n",
              " ':',\n",
              " 'I',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " ',',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " ',',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':',\n",
              " '(',\n",
              " '#',\n",
              " 'amiright']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "nltk.word_tokenize(tweet, language='spanish')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More options..."
      ],
      "metadata": {
        "id": "-KBibnjA1b26"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PDcB89vWvITD"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "custom_tokenizer = RegexpTokenizer('[a-zA-Z0-9]*', discard_empty=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbfG3liKvITE",
        "outputId": "b5243325-f4d3-4984-a062-8354c28743f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " '',\n",
              " '',\n",
              " 'lOR42wsOEFcv3f',\n",
              " '',\n",
              " '',\n",
              " 'I',\n",
              " '',\n",
              " 'fall',\n",
              " '',\n",
              " 'too',\n",
              " '',\n",
              " 'fast',\n",
              " '',\n",
              " '',\n",
              " 'crash',\n",
              " '',\n",
              " 'too',\n",
              " '',\n",
              " 'hard',\n",
              " '',\n",
              " '',\n",
              " 'forgive',\n",
              " '',\n",
              " 'too',\n",
              " '',\n",
              " 'easily',\n",
              " '',\n",
              " 'and',\n",
              " '',\n",
              " 'care',\n",
              " '',\n",
              " 'too',\n",
              " '',\n",
              " 'much',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'amiright',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "custom_tokenizer.tokenize(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BUojQmtRvITE"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jm8kis0bvITE",
        "outputId": "9c6e559e-2860-4604-fe1d-3abb8aa66ae7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " ':',\n",
              " 'I',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " ',',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " ',',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(',\n",
              " '#amiright']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "tweet_tokenizer.tokenize(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yltAQxCvITE",
        "outputId": "8af56d02-158e-40b8-bd24-d83252d1c348"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " ':',\n",
              " 'I',\n",
              " 'fall',\n",
              " 'too_fast',\n",
              " ',',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " ',',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(',\n",
              " '#amiright']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "mwe = MWETokenizer()\n",
        "mwe.add_mwe(('too', 'fast'))\n",
        "mwe.tokenize(tweet_tokenizer.tokenize(tweet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8fDVPDRnvITF"
      },
      "outputs": [],
      "source": [
        "mwe.add_mwe((('too', 'fast'), ('too', 'hard')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugIC2npavITF",
        "outputId": "ba4ee8c6-d498-4fec-c07e-418d8852609c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "query = 'fast'\n",
        "query in mwe.tokenize(tweet_tokenizer.tokenize(tweet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj-5pXTyvITG"
      },
      "source": [
        "### Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bRK61fGevITG",
        "outputId": "a8889f90-7331-46b4-f1d8-91c12f6a87df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'rt @lor42wsoefcv3f: i fall too fast, crash too hard, forgive too easily and care too much... :( #amiright'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "tweet.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "g4-6Rnr7vITG"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def normalize_tokens(tokenized_text):\n",
        "    # Lowercase\n",
        "    tokens = [t.lower() for t in tokenized_text]\n",
        "    # Remove hashtags\n",
        "    tokens = [t for t in tokens if not t.startswith('#')]\n",
        "    # Remove punctuation\n",
        "    tokens = [t for t in tokens if t not in string.punctuation]\n",
        "    # Keep only letters\n",
        "#     tokens = [t for t in tokens if re.match('^[a-z]+$', t)]\n",
        "    # Normalize characters\n",
        "#     tokens = [re.sub('á', 'a', t) for t in tokens]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edRDrifZvITG",
        "outputId": "8af8a63d-a399-40ab-d3e3-f36137cac49d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['muy', 'rápido']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "spanish_query = 'muy rápido'\n",
        "normalize_tokens(tweet_tokenizer.tokenize(spanish_query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "HFpAR4b_vITH",
        "outputId": "7fb53869-6b00-49d2-9bcb-a676253dac3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 28.3 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 29.4 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 19.5 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 71 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 81 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 92 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 112 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 122 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 133 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 143 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 153 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 163 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 174 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 184 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 194 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 204 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 215 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 225 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 6.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'muy rapido'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "!pip install unidecode\n",
        "import unidecode\n",
        "unidecode.unidecode(spanish_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR5AUDgevITH",
        "outputId": "5444c712-ef73-416a-b144-15b077650c9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt',\n",
              " 'i',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "normalize_tokens(tweet_tokenizer.tokenize(tweet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLG3Pd4SvITH"
      },
      "source": [
        "#### Uniform normalization principle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "491ipWWuvITH",
        "outputId": "5f5f7031-0dbc-4610-b680-4ae74407bfca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['too', 'fast', 'too', 'furious']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "query = 'TOO fast TOO furious'\n",
        "tokenized_query = tweet_tokenizer.tokenize(query)\n",
        "normalized_query = normalize_tokens(tokenized_query)\n",
        "# normalized_query = tokenized_query\n",
        "normalized_query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfpl7C7nvITH",
        "outputId": "b91a8c80-234d-4913-a0cf-592fadd83a0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt',\n",
              " 'i',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "normalized_tweet = normalize_tokens(tweet_tokenizer.tokenize(tweet))\n",
        "# normalized_tweet = normalize_tokens(tweet.split())\n",
        "normalized_tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "truJmsHpvITI",
        "outputId": "ffd377ba-a04c-49b5-8585-93624edbcbd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'too', 'fast'}\n",
            "2 common word(s)\n"
          ]
        }
      ],
      "source": [
        "common_words = set(normalized_query).intersection(normalized_tweet)\n",
        "print(common_words)\n",
        "print(len(common_words), \"common word(s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaLxOlOMvITJ"
      },
      "source": [
        "#### Stemming / Lemmatization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "2pmCSIy4vITJ"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoXs3q-SvITJ",
        "outputId": "e69eaf0f-2bfe-4275-df83-43b7514202c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt',\n",
              " 'i',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " 'forgiv',\n",
              " 'too',\n",
              " 'easili',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "[stemmer.stem(t) for t in normalized_tweet]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = nltk.LancasterStemmer() # is prone to overstemming\n",
        "[stemmer.stem(t) for t in normalized_tweet]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mWZSR5d4OuH",
        "outputId": "c333a582-3f16-4ffd-e0ed-503694731373"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt',\n",
              " 'i',\n",
              " 'fal',\n",
              " 'too',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " 'forg',\n",
              " 'too',\n",
              " 'easy',\n",
              " 'and',\n",
              " 'car',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xxv9vv3TvITK",
        "outputId": "393adbd6-4da9-4c3a-a6ae-518bcf86146a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt',\n",
              " 'i',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " 'forgiv',\n",
              " 'too',\n",
              " 'easili',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "stemmer = SnowballStemmer(language='english') # Porter2\n",
        "\n",
        "[stemmer.stem(t) for t in normalized_tweet]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stemmer.stem(\"running\"))\n",
        "\n",
        "print(stemmer.stem(\"runs\"))\n",
        "\n",
        "print(stemmer.stem(\"ran\"))\n",
        "\n",
        "print(stemmer.stem(\"darling\"))\n",
        "\n",
        "print(stemmer.stem(\"are\"))\n",
        "\n",
        "print(stemmer.stem(\"bring\"))\n",
        "\n",
        "print(stemmer.stem(\"being\"))\n",
        "\n",
        "print(stemmer.stem(\"Charles\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEDgnkOE4pza",
        "outputId": "d9d9fc03-d740-449e-f585-1592735f1d92"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "run\n",
            "ran\n",
            "darl\n",
            "are\n",
            "bring\n",
            "be\n",
            "charl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1caMRHT7vITK",
        "outputId": "36c51fe5-bed6-43d7-f72a-7e236cff5912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt',\n",
              " 'i',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "[lemmatizer.lemmatize(t) for t in normalized_tweet]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9yhe4EZvITK",
        "outputId": "fbd848c0-2ba7-499a-aeaf-a9d23ad1bb33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[('rt', 'NN'), ('i', 'NN'), ('fall', 'VBP'), ('too', 'RB'), ('fast', 'JJ'), ('crash', 'NN'), ('too', 'RB'), ('hard', 'JJ'), ('forgive', 'JJ'), ('too', 'RB'), ('easily', 'RB'), ('and', 'CC'), ('care', 'VB'), ('too', 'RB'), ('much', 'JJ'), ('...', ':'), (':(', 'NN')]\n"
          ]
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "tagged_tweet = nltk.pos_tag(normalized_tweet)\n",
        "print(tagged_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Yww_9WdPvITK"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "tag_map = {'J': wn.ADJ, 'V': wn.VERB, 'R': wn.ADV, 'N': wn.NOUN}\n",
        "def get_lemmas(tokenized_text):\n",
        "    tagged_text = nltk.pos_tag(tokenized_text)\n",
        "    return [lemmatizer.lemmatize(w, pos=tag_map.get(p[0], wn.NOUN)) for (w, p) in tagged_text]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwkQvd-SvITL",
        "outputId": "c04e41d1-462b-4149-b71c-73b6352e3626"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'fastest']\n"
          ]
        }
      ],
      "source": [
        "query = \"the fastest!\"\n",
        "normalized_query = normalize_tokens(tweet_tokenizer.tokenize(query))\n",
        "print(normalized_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8AjyEA5vITL",
        "outputId": "23f2a1a4-db47-4ada-bf3b-4f3e9f5dffc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['rt', 'i', 'fall', 'too', 'fast', 'crash', 'too', 'hard', 'forgive', 'too', 'easily', 'and', 'care', 'too', 'much', '...', ':(']\n",
            "['the', 'fast']\n"
          ]
        }
      ],
      "source": [
        "lemmatized_tweet = get_lemmas(normalized_tweet)\n",
        "lemmatized_query = get_lemmas(normalized_query)\n",
        "print(lemmatized_tweet)\n",
        "print(lemmatized_query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R08IvRkUvITL",
        "outputId": "eed97b08-88dc-4e08-b6ad-cd9336ed5a34"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'am', 'so', 'fast', 'i', 'am', 'the', 'fastest']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "tweet = \"I am so fast, I am the fastest!\"\n",
        "normalized_tweet = normalize_tokens(tweet_tokenizer.tokenize(tweet))\n",
        "normalized_tweet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[lemmatizer.lemmatize(t) for t in normalized_tweet]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P9ov1TJ67Ml",
        "outputId": "bcc1e0f8-5475-42ca-fc55-918714b65141"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'am', 'so', 'fast', 'i', 'am', 'the', 'fastest']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24QRJbnIvITM",
        "outputId": "b6b08b3c-883e-40d2-fc6d-413798cd9c9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'be', 'so', 'fast', 'i', 'be', 'the', 'fast']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "get_lemmas(normalized_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq3JncFSvITL",
        "outputId": "671cd4df-52ac-41d6-d0bb-c2142ebd915f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common words: {'fast'}\n"
          ]
        }
      ],
      "source": [
        "print(\"Common words:\", set(lemmatized_tweet).intersection(set(lemmatized_query)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exJwCbhSvITI"
      },
      "source": [
        "#### Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R7abjAkvITI",
        "outputId": "3e125fae-ae2c-43ad-e6a7-6038af328861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu8sWb4vvITI",
        "outputId": "e05032ea-ee79-41bc-af0e-c0df805723ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "oNzURg2kvITJ"
      },
      "outputs": [],
      "source": [
        "blacklist_words = stopwords.words('english') + ['rt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-0g081PvITJ",
        "outputId": "a7becb23-0bdc-43ed-b013-aa60eb39f58e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fast', 'fastest']\n"
          ]
        }
      ],
      "source": [
        "cleaned_tweet = [t for t in normalized_tweet if t not in blacklist_words]\n",
        "print(cleaned_tweet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnD0SkZNvITM"
      },
      "source": [
        "#### Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xckCNborvITM",
        "outputId": "d33a9f38-124f-4856-b32d-cd8fa19fe6ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 2), ('be', 2), ('fast', 2), ('so', 1), ('the', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "Counter(get_lemmas(normalized_tweet)).most_common(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOLFMVSFvITM",
        "outputId": "d11330b4-0162-4248-f710-30d267d1a87c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'be', 'so', 'fast', 'i', 'be', 'the', 'fast']\n"
          ]
        }
      ],
      "source": [
        "tweet = \"I am so fast, I am the fastest!\"\n",
        "normalized_tweet = normalize_tokens(tweet_tokenizer.tokenize(tweet))\n",
        "lemmatized_tweet = get_lemmas(normalized_tweet)\n",
        "print(lemmatized_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR5W_IeqvITM",
        "outputId": "7935fe99-226a-419f-cceb-ddb9bc740c24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'i': 2, 'am': 2, 'so': 1, 'fast': 1, 'the': 1, 'fastest': 1})\n",
            "Counter({'i': 2, 'be': 2, 'fast': 2, 'so': 1, 'the': 1})\n"
          ]
        }
      ],
      "source": [
        "print(Counter(normalized_tweet))\n",
        "print(Counter(lemmatized_tweet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VIE2dhCvITM"
      },
      "source": [
        "#### Sentence segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "zDhiS2VGvITN"
      },
      "outputs": [],
      "source": [
        "query = \"I am too fast. I am too furious.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "FPADLll5vITN"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Pp_45fMvITN",
        "outputId": "57c6dd2d-1016-4f37-d1d5-79bb4a03160d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I am too fast.', 'I am too furious.']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "sent_tokenize(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rrVvihFvITN",
        "outputId": "77532976-d8af-4dd1-81d0-691e506b96d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Soy muy rápido!', 'Estoy muy furioso!']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')\n",
        "spanish_query = 'Soy muy rápido! Estoy muy furioso!'\n",
        "spanish_tokenizer.tokenize(spanish_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PuN1DjvvITN",
        "outputId": "d5ba67d0-9b22-4a27-e79b-781561f11488"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['J.K. Rowling is rich.', 'I am not as rich as J.K.']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "sent_tokenize(\"J.K. Rowling is rich. I am not as rich as J.K.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "BJaD2GydvITO"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "PunktSentenceTokenizer??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mcm9gIBvITO"
      },
      "source": [
        "#### Numeral conversion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install word2number\n",
        "!pip install num2word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuRsqSRbzuDk",
        "outputId": "1efeaaa1-639b-4faf-e4a6-b82f9047cf03"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting word2number\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "Building wheels for collected packages: word2number\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5582 sha256=e04c813555acf342b58dd89f5a46806f61679fda98124a00071a9c628437aab5\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/c3/77/a5f48aeb0d3efb7cd5ad61cbd3da30bbf9ffc9662b07c9f879\n",
            "Successfully built word2number\n",
            "Installing collected packages: word2number\n",
            "Successfully installed word2number-1.1\n",
            "Collecting num2word\n",
            "  Downloading num2word-1.0.1-py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: num2word\n",
            "Successfully installed num2word-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbn8ybBLvITO",
        "outputId": "9f789737-3c23-4ac8-bbf1-0f7eab99dc87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "import word2number\n",
        "from word2number import w2n\n",
        "w2n.word_to_num(\"eleven\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2n.word_to_num(\"twenty three\")"
      ],
      "metadata": {
        "id": "vpA1EUb5z-CT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bddae3e-a4f0-4e49-b352-c34c75c22226"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "couI08gLvITO",
        "outputId": "5e078973-02e4-4ee2-bc9a-da42d8bc0fc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting num2words\n",
            "  Downloading num2words-0.5.10-py3-none-any.whl (101 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▎                            | 10 kB 30.6 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20 kB 30.4 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 30 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 51 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 61 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 71 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 92 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 101 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words) (0.6.2)\n",
            "Installing collected packages: num2words\n",
            "Successfully installed num2words-0.5.10\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'twelve'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "!pip install num2words\n",
        "\n",
        "import num2words\n",
        "num2words.num2words(12)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num2words.num2words(101)"
      ],
      "metadata": {
        "id": "mjtdQ_n60B2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "0b30b050-2f4e-43ff-adda-778ec16f17e5"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'one hundred and one'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num2words.num2words(2020)"
      ],
      "metadata": {
        "id": "YAK0RUY_0Dnn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "fdd1bd11-645e-4bab-ba4e-e7c8699fbe48"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'two thousand and twenty'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2n.word_to_num(\"Twelve o'clock!\")"
      ],
      "metadata": {
        "id": "wCvn9L_W7TDX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42f27951-de13-472c-c808-7a4f67425f61"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise"
      ],
      "metadata": {
        "id": "lM4HWS8I_hUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find a recent news article online.\n",
        "Read it in a python variable (input it manually or read from a file).\n",
        "\n",
        "Write a function that normalizes the text and splits it into tokens. Add flags to customize the different preprocessing choices (which stemmer/lemmatizer to use, whether to lowercase, whether to convert numbers, whether to remove stopwords, ...). \n",
        "\n",
        "Store the vocabulary of unique tokens found in the text.\n",
        "\n",
        "Compare the number of unique tokens (\"types\") with different preprocessing settings."
      ],
      "metadata": {
        "id": "7VJBKi1B_lZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The data is pre-processed in multiple steps as follows:\n",
        "\n",
        "*   Emoticons and emojis are replaced with the corresponding words.\n",
        "*   The URL addresses are replaced by the <URL> token. Hyperlinks and HTML tags are removed, as well as the old-style for highlighting redistributed tweets\n",
        "*   We eliminate all email addresses.\n",
        "*   The text is decoded and then normalized, i.e., the data is transformed from complex symbols into simple characters. Characters can be subjected to various forms of encoding, such as Latin, ISO/IEC 8859-1, etc. Therefore, for better analysis, it is necessary to keep the data in a standard encoding format. For this requirement, we choose UTF-8 encoding because it is widely accepted and often recommended.\n",
        "*   Bounded words are separated by inserting a space if the user wants to keep them, otherwise they will be deleted along with the hashtags. Most posts on social networks such as Facebook, Twitter, or Instagram contain one or more words without spaces and are preceded by the # sign such as #MentalHealthAwarenessWeek or #BeautifulDay, called a hashtag. A hashtag is a tag that makes it easy to find posts in a specific category or with certain content. Therefore, the words in the hashtags provide essential information about the general feeling of the user, predominant topic etc. \n",
        "*   Lower case\n",
        "*   Tokenization\n",
        "*   Any letter repeated more than three times in a row is replaced by two repetitions of the same letter as the usual rules of English spelling forbid triple letters (for example \"cooool\" is replaced by \"cool\").\n",
        "*   The user can decide whether the stopwords should be removed or not.\n",
        "*   The user can decide whether punctuation and/or numeric characters should be removed. Also, in the case of keeping numeric characters, the user can choose to convert them into specific words.\n",
        "*   The user can choose to perform the stemmatization or lemmatization process.\n"
      ],
      "metadata": {
        "id": "oM7ZdpqXA0l4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bs4\n",
        "!pip install emoji"
      ],
      "metadata": {
        "id": "ga37tHqo_jqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf4110b-3939-469c-d59a-6bcf2d84706c"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 7.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=15e2c4e2a0c190266ae6a9cf8defda06397e03ad12f4da80246068f137986d69\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize\n",
        "from emoji.core import demojize\n",
        "\n",
        "import string, re, unicodedata\n",
        "import word2number \n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "Gu6KSo1X_PpJ"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mail_reg = r'^(\\w|\\.|\\_|\\-)+[@](\\w|\\_|\\-|\\.)+[.]\\w{2,3}$'\n",
        "url_addresses_reg = r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?|http:/\\W\"|http:\\/\\/\\w\\W\\.\\W'\n",
        "retweets_reg = r'^rt[\\s]+|rt '"
      ],
      "metadata": {
        "id": "kBu8m1ATKmZQ"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove HTML tags\n",
        "def remove_html(data):\n",
        "  bs = BeautifulSoup(data, 'html.parser')\n",
        "  return bs.get_text()\n",
        "\n",
        "# remove URL addresses\n",
        "def remove_url(data):\n",
        "  return re.sub(url_addresses_reg, ' ', data)\n",
        "\n",
        "# remove email addresses\n",
        "def remove_email_addresses(data):\n",
        "  return re.sub(mail_reg, ' ', data)"
      ],
      "metadata": {
        "id": "73ixn5x_KzJa"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMOTICONS = [\n",
        "    (\"Laughing\", r':[‑,-]?\\){2,}'),\n",
        "    (\"Rolling_on_the_floor_laughing\", r'\\=\\){2,}|\\=\\]'),\n",
        "    (\"Heart\", r'<3'),\n",
        "    (\"Broken_heart\", r'<\\\\3'),\n",
        "    ('Very_happy', r':[‑,-]?D'),\n",
        "    ('Happy_face_or_smiley', r'[:,8,=][‑,-,o,O]?\\)|\\(\\^[v,u,o,O]\\^\\)|:[‑,-]?3'),\n",
        "    ('Happy', r'=]'),\n",
        "    ('Mischievous_smile', r':[‑,-]?>'),\n",
        "    ('Sticking_tongue_out_playfulness_or_cheekiness', r':P|:[‑,-]P|;P|:b|:-b'),\n",
        "    ('Kiss', r':[‑,-]?[\\*,X,x]'),\n",
        "    ('Joy', r' uwu | UwU '),\n",
        "    ('Surprised_or_shock', r':[‑,-]?[o|O|0]|o_O|o_0'),\n",
        "    ('Sad_frown_andry_or_pouting', r':[‑,-]?\\('),\n",
        "    ('Very_sad', r':[(]{2,}'),\n",
        "    ('Crying', r':[‑,-]?\\'\\('),\n",
        "    ('Straight_face_no_expression_dissaproval_or_not_funny', r':[‑,-]?\\|'),\n",
        "    ('Annoyed_or_hesitant', r'>?[:][\\\\|\\/]|\\=\\/|=\\\\'),\n",
        "    ('Angel_saint_or_innocent', r'[0,O,o]:[‑,-]?[\\),3]'),\n",
        "    ('Embarrassed_or_blushing', r':\\$'),\n",
        "    ('Sad_or_crying', r';_;|\\(;_;\\)|\\(\\'_\\'\\)|Q_Q|\\(;_:\\)|\\(:_;\\)'),\n",
        "    ('Evil_or_devilish', r'[>|}|3]:[‑,-]?\\)'),\n",
        "    ('Laughing_big_grin_or_laugh_with_glasses', r'[:,8,X,=][-,‑]?[D,3]|B\\^D'),\n",
        "    ('Tears_of_happiness', r':[\\',\\`][‑,-]?\\)'),\n",
        "    ('Horror', r'D[-,‑]\\''),\n",
        "    ('Great_dismay', r'D[8,;,=]'),\n",
        "    ('Tongue_in_cheek', r':[-,‑]J'),\n",
        "    ('Yawn', r'8[‑,-]0|>:O'),\n",
        "    ('Sadness', r'D:'),\n",
        "    ('Disgust', r'D:<'),\n",
        "    ('Cool', r'\\|;[‑,-]\\)'),\n",
        "    ('Drunk_or_confused', r'%[-,‑]?\\)'),\n",
        "    ('Sealed_lips_or_wearing_braces_or_tongue_tied', r':[-,‑]?[x,#,&]'),\n",
        "    ('Skeptical_annoyed_undecided_uneasy_or_hesitant', r':[-,‑]?[.,/]|:[L,S]|=[/,L]'),\n",
        "    ('Scepticism_disbelief_or_disapproval', r'\\',:-\\||\\',:[-,-]'),\n",
        "    ('Party_all_night', r'#‑\\)'),\n",
        "    ('Headphones_listening_to_music', r'\\(\\(d\\[-_-\\]b\\)\\)'),\n",
        "    ('Bored', r'\\|‑O'),\n",
        "    ('Dump', r'<:‑\\|'),\n",
        "    ('Being_sick', r':-?#{2,3}..'),\n",
        "    ('Amazed', r'\\(\\*_\\*\\)|\\(\\+_\\+\\)|\\(\\@_\\@\\)'),\n",
        "    ('Confusion', r'\\(\\?_\\?\\)|\\(\\・\\・?'),\n",
        "    ('Wink_or_smirk', r';[-,‑]?[\\),D,\\]]|\\*[-,‑]?\\)|;\\^\\)|:‑,|;3'),\n",
        "    ('Exciting', r'\\\\\\(\\^o\\^\\)\\/|\\\\\\(\\^o\\^\\)\\／|ヽ\\(\\^o\\^\\)丿|\\(\\*^0^\\*\\)|＼\\(-o-\\)／|＼\\(~o~\\)\\／'),\n",
        "    ('Giggling_with_hand_covering_mouth', r'\\^m\\^'),\n",
        "    ('Joyful', r'\\(\\^_\\^\\)/|\\(\\^[O,o]\\^\\)／|\\(°o°\\)'),\n",
        "    ('Tired', r'\\(=_=\\)'),\n",
        "    ('Shame', r'\\(-_-\\)|\\(一_一\\)'),\n",
        "    ('Surprised', r'\\(o\\.o\\)'),\n",
        "    ('Sleeping', r'\\(-_-\\)zzz'),\n",
        "    ('Kowtow_as_a_sign_of_respect_or_dogeza_for_apology', r'\\(__\\)|_\\(\\._\\.\\)_|<\\(_ _\\)>|m\\(_ _\\)m|m\\(__\\)m|<m\\(__\\)m>|_\\(_\\^_\\)_'),\n",
        "    ('Troubled', r'\\(>_<\\)>?'),\n",
        "    ('Nervous__Embarrassed_Troubled_Shy_Sweat_drop', r'\\(-_-;\\)|\\(\\^_\\^;\\)|\\(-_-;\\)|\\(~_~;\\)|\\(・.・;\\)|\\(・_・;\\)'),\n",
        "    ('Wink', r'\\(\\^_-\\)'),\n",
        "    ('Normal_laugh', r'>\\^_\\^<|<\\^!\\^>|\\(\\^\\.\\^\\)|\\(\\^J\\^\\)|\\(\\*\\^[_,.]\\^\\*\\)|\\(\\^<\\^\\)|\\(\\^\\.\\^\\)|\\(#\\^\\.\\^#\\)'),\n",
        "    ('STH_ELSE', r'.')\n",
        "]\n",
        "\n",
        "emoticons_tokens = '|'.join('(?P<%s>%s)' % emoticon for emoticon in EMOTICONS)\n",
        "\n",
        "def replace_emoticons(text):\n",
        "    new_text = \"\"\n",
        "    for match in re.finditer(emoticons_tokens, text):\n",
        "        emoticon_name = match.lastgroup\n",
        "        emoticon = match.group(emoticon_name)\n",
        "        if emoticon_name == 'STH_ELSE':\n",
        "            new_text += emoticon\n",
        "        else:\n",
        "            new_text += emoticon_name\n",
        "    return new_text"
      ],
      "metadata": {
        "id": "Gf6o8xsKDC-M"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(x):\n",
        "    x = unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore') # remove accented chars\n",
        "    return x"
      ],
      "metadata": {
        "id": "K_xiUmizDVVO"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# any letter repeated more than three times in a row is replaced by two repetitions of the same letter\n",
        "def remove_multiple_occurences(text):\n",
        "    n = len(text)\n",
        "\n",
        "    if n < 3:\n",
        "        return text\n",
        "\n",
        "    i, count = 0, 0\n",
        "    while i < n - 1:\n",
        "        i += 1\n",
        "        if text[i] != text[i-1]:\n",
        "            count = 0\n",
        "        else:\n",
        "            count += 1\n",
        "            if count >= 2:\n",
        "                text = text[:i] + text[i+1:]\n",
        "                n -= 1\n",
        "                i -= 1\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "DKx8EtMDDbyG"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "lemmatizaer = WordNetLemmatizer()\n",
        "\n",
        "def preprocessing_text(text, language=\"english\", remove_stopwords=False, remove_punctuation=False, remove_nums=False, convert_nums=True, stemming=False, lemmatization=False):\n",
        "  # Remove URL addresses, tags, retweets, and email addresses\n",
        "  preprocessed_text = remove_url(text)\n",
        "  preprocessed_text = remove_html(preprocessed_text)\n",
        "  preprocessed_text = remove_email_addresses(preprocessed_text)\n",
        "  preprocessed_text = re.sub(retweets_reg, ' ', preprocessed_text)\n",
        "  # Lowecasing\n",
        "  preprocessed_text = preprocessed_text.lower()\n",
        "  # Tokenize words\n",
        "  tokens = word_tokenize(preprocessed_text)\n",
        "  # Conversion of emojis to words\n",
        "  tokens = [demojize(token) for token in tokens]\n",
        "  # Conversion of emoticons to words\n",
        "  tokens = [replace_emoticons(token) for token in tokens]\n",
        "  # Normalize text\n",
        "  preprocessed_text = unicodedata.normalize('NFKD', preprocessed_text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "  # Remove repeated letters from words\n",
        "  tokens = [remove_multiple_occurences(token) for token in tokens]\n",
        "  # Remove STOPWORDS\n",
        "  if remove_stopwords:\n",
        "    tokens = [token for token in tokens if token is not None and token not in stopwords.words(language)]\n",
        "  # Remove punctuations\n",
        "  if remove_punctuation:\n",
        "    tokens = [token for token in tokens if token is not None and token not in string.punctuation]\n",
        "  # Remove numerical_characters\n",
        "  if remove_nums:\n",
        "    tokens = [token for token in tokens if token is not None and token not in string.digits]\n",
        "  else:\n",
        "    if convert_nums:\n",
        "      # Conversion of numerical characters to words\n",
        "      tokens = [num2words.num2words(float(token)) if (token.isdigit() or token.isdecimal()) else token for token in tokens]\n",
        "\n",
        "  if stemming:\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "  if lemmatization:\n",
        "    tokens = [lemmatizaer.lemmatize(token) for token in tokens]\n",
        "\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "P0Se9JhVA7wf"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oynZyQfDD_Va",
        "outputId": "47b712dc-0617-41cf-d453-309d304bc9fe"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/news_article.txt') as f:\n",
        "    text = f.read()\n",
        "    print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Jn7X8fKTIlj",
        "outputId": "091b8fee-2377-4416-9e52-f99f511322fe"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Yankee\n",
            "From Wikipedia, the free encyclopedia\n",
            "Jump to navigationJump to search\n",
            "This article is about a literary magazine published 1828–1829. For the magazine founded in 1935, see Yankee (magazine).\n",
            "The Yankee\n",
            "Stained white paper with words printed in three columns in blank ink below the magazine title and motto\n",
            "First page of the first issue: January 1, 1828\n",
            "Editor\tJohn Neal\n",
            "Categories\tLiterature, gymnastics, New England, England, art, theater, politics, utilitarianism, women's rights\n",
            "Frequency\tWeekly (January 1, 1828 – July 3, 1829)\n",
            "Monthly (July–December 1829)\n",
            "Founder\tJohn Neal\n",
            "First issue\tJanuary 1, 1828\n",
            "Final issue\tDecember 1829\n",
            "Based in\tPortland, Maine, US\n",
            "The Yankee (later retitled The Yankee and Boston Literary Gazette) was one of the first cultural publications in the US, founded and edited by John Neal (1793–1876), and published in Portland, Maine. The magazine was unique at the time for its independent journalism. Neal used creative control of the magazine to improve his social status, help establish the American gymnastics movement, cover national politics, and criticize American literature, art, theater, and social issues.\n",
            "\n",
            "Many new, predominantly female, writers and editors started their careers with contributions and criticism of their work published in The Yankee, including many who are familiar to modern readers. Essays by Neal on American art and theater anticipated major changes and movements in those fields realized in the following decades. The articles on women's rights and early feminist ideas affirmed intellectual equality between men and women and demanded political and economic rights for women. Conflicting opinions published in The Yankee on the cultural identity of Maine and New England presented readers with a complex portrait of the region. The magazine began as a weekly publication and later converted to a longer, monthly format. Its two-year run concluded at the end of 1829.\n",
            "\n",
            "\n",
            "Contents\n",
            "1\tBackground\n",
            "2\tContent\n",
            "2.1\tLiterary criticism\n",
            "2.2\tArt criticism\n",
            "2.3\tTheatrical criticism\n",
            "2.4\tPolitical, social, and civic issues\n",
            "2.5\tFeminism\n",
            "2.6\tNew England\n",
            "2.7\tFeuds\n",
            "3\tRun of publication\n",
            "4\tReferences\n",
            "4.1\tCitations\n",
            "4.2\tSources\n",
            "5\tExternal links\n",
            "Background\n",
            "Color oil painting of the bust of a young white man with a white shirt, dark jacket, and cloak\n",
            "John Neal in 1823\n",
            "John Neal grew up in Portland, Maine (then the District of Maine), and later lived in Boston, then Baltimore, where he pursued a dual career in law and literature following the bankruptcy of his dry goods business in 1816.[1] After gaining national recognition as a critic, poet, and novelist,[2] he sailed to London, where he wrote for British magazines and served as Jeremy Bentham's personal secretary.[3] Upon returning to his native Portland in 1827, he was confronted by community members who were offended by Neal's literary work in the preceding years: the unsympathetic depiction of his hometown in his semi-autobiographical novel Errata (1823), the way he depicted New England dialect and customs in his novel Brother Jonathan (1825), and his criticism of American writers in Blackwood's Magazine (1824–1825).[4] Residents posted inflammatory broadsides calling Neal \"a panderer for scandal against the country that nourished him\" and a \"renegado\" who \"basely traduced his native town and country for hire\".[5] Neal experienced verbal taunting and physical violence in the streets[6] and an attempt to block his admission to the local bar association, though he had been a practicing lawyer in Baltimore (1820–1823).[7] In the second half of 1827, he pursued a number of projects to further his personal goals and to vindicate himself to his local community. He joined the bar despite opposition,[8] founded Maine's first athletic program,[9] and established The Yankee.[10] The first issue was published January 1, 1828.[11]\n",
            "\n",
            "The idea came from a local bookseller who urged Neal shortly after his return to Portland to establish a new magazine or newspaper. Neal initially refused, not wanting to be the financial backer of his own literary undertaking. The bookseller then offered to publish the periodical if Neal would serve as editor, which Neal accepted.[12] Subscription to the new weekly magazine cost $3 a year, or $2.50 paid in advance.[13]\n",
            "\n",
            "The Yankee was Maine's first literary periodical[8] and one of America's first cultural publications.[14] Controversial at the time for its lack of association with any political party or interest group,[13] it was a precursor for the independent American press that was established later in the century.[15] When asked why he would establish such a magazine outside a major city, Neal said, \"We mean to publish in Portland. Whatever the people of New-York, or Boston or Philadelphia or Baltimore might say, Portland is the place for us.\"[16]\n",
            "\n",
            "Content\n",
            "See also: Articles by John Neal\n",
            "The Yankee functioned to educate Americans about England, spread Jeremy Bentham-inspired utilitarian philosophy, publish literary contributions, and critique American literature, American art, theater, politics, and social issues.[17] The magazine also aided in establishing the US gymnastics movement,[18] provided a forum for new writers,[19] and promoted Neal's own accomplishments.[20] Because Neal included a high proportion of his own work, self-promotion, and details of feuds with other public figures, \"no magazine ever bore more fully the stamp of a personality\",[20] according to scholar Irving T. Richards. Other authors published in the magazine included John Greenleaf Whittier,[21] Edgar Allan Poe,[22] Albert Pike (later associate justice of the Arkansas Supreme Court),[22] Grenville Mellen,[22] Isaac Ray,[23] and early published works[24] by John Appleton (later chief justice of Maine).[22]\n",
            "\n",
            "Literary criticism\n",
            "Stained white paper with words printed in three columns in blank ink below the magazine title and motto\n",
            "The first issue with the new name: August 20, 1828\n",
            "Neal biographer Donald A. Sears felt that The Yankee's greatest impact was encouraging new authors through publication and criticism of their early works.[25] Poe, Whittier, Nathaniel Hawthorne, and Henry Wadsworth Longfellow all received their first impactful encouragement in its pages.[21] Most of the new authors whose careers started in The Yankee were women, including Elizabeth Oakes Smith and others lesser known to history.[26]\n",
            "\n",
            "The Yankee is credited with having \"discovered\"[26] Poe, and influenced the young writer's style with the magazine's essays.[27] Poe considered Neal's September 1829 review of the poem \"Fairy-Land\" to be \"the very first words of encouragement I ever remember to have heard\".[28] Poe became a contributor to the Ladies' Magazine shortly afterward – a relationship that may have been orchestrated by Neal.[29] Whittier sought Neal's opinion in the magazine at a turning point in the poet's career, saying when he submitted a poem that \"if you don't like it, say so privately; and 'I will quit poetry, and everything also of a literary nature, for I am sick at heart of the business'.\"[30] In what may be the first review of Hawthorne's first novel, The Yankee referred to Fanshawe as \"powerful and pathetic\" and said that the author \"should be encouraged to persevering efforts by a fair prospect of future success\".[31] An 1828 review of Longfellow noted \"a fine genius and a pure and safe taste\" but also cited the need for \"a little more energy, and a little more stoutness\".[32]\n",
            "\n",
            "Art criticism\n",
            "Neal was the first American art critic.[33] Scholars find his work in the novel Randolph (1823), Blackwood's Magazine (1824), and The Yankee to be the most historically important,[34] in which he discussed leading American artists and their work \"with unprecedented acumen and enthusiasm\".[35] The essay \"Landscape and Portrait-Painting\" (September 1829) anticipated John Ruskin's groundbreaking Modern Painters (1843) by distinguishing between \"things seen by the artist\" and \"things as they are\", as Ruskin put it more famously fourteen years later.[36] In Neal's words in 1829, \"There is not a landscape nor a portrait painter alive who dares to paint what he sees as he sees it; nor probably a dozen with power to see things as they are.\"[37]\n",
            "\n",
            "Neal's essays in The Yankee about landscape painting and its potential role in America's artistic renaissance anticipate the rise of the Hudson River School and provide early coverage (1828) of its founders, Thomas Doughty, Asher Brown Durand, and Thomas Cole.[38] These essays also offer unprecedented coverage of reproduction technology like engraving and lithography[39] and American portrait painters trained in the \"humbler contingencies\"[40] of sign painting and applied arts.[41] According to art scholar Harold E. Dickson, Neal's opinions in The Yankee \"to a remarkable degree ... have stood the trying test of time.\"[42]\n",
            "\n",
            "Theatrical criticism\n",
            "At the time The Yankee was in circulation, Neal was one of the most important critics of American drama.[43] His serial essay \"The Drama\" (July–December 1829) elaborates upon opinions on theater originally published in the prefaces to his first play, Otho (1819) and his second poetry collection, The Battle of Niagara: Second Edition (1819).[44] The essay dismissed well-accepted Shakespearean standards and outlined a prophecy for the future American drama that largely played out by the end of the century.[45] Neal predicted that characters would become more relatable by expressing feelings \"in common language\"[46] because \"when a person talks beautifully in his sorrow, it shows both great preparation and insincerity.\"[47] Instead of relying on highly cultivated circumstances in the plot, \"The incidents will be such as every man may hope or dread to see ...; for it is there, and there only, that we can judge of a hero, or of a nation, or sympathize with either.\"[48] This \"thorough revolution in plays and players, authors and actors\"[49] called for in \"The Drama\" was still in process 60 years later when William Dean Howells was considered innovative for issuing the same criticism.[50]\n",
            "\n",
            "Political, social, and civic issues\n",
            "Black and white engraving of a grand Greek revival civic building before an open space paved with dirt and granite\n",
            "Neal's gymnasium in Portland's town hall at Market Square was Maine's first athletic program.[9]\n",
            "The Yankee documented and offered commentary upon the period's nationally relevant social and political topics, such as the nullification crisis, the Tariff of Abominations, Andrew Jackson's spoils system, lotteries, temperance, women's rights, and the Maine-New Brunswick border issues that led to the 1838–1839 Aroostook War between the United States and the United Kingdom.[51] He published a \"vigorous campaign\" of seventeen articles against lotteries over the course of 1828, claiming they encourage idle and reckless behavior among patrons.[52] On a local level, Neal's advocacy in The Yankee contributed toward municipal funding being designated for the construction of Portland's first sidewalks.[53]\n",
            "\n",
            "In March 1828, Neal advertised his gymnasium in The Yankee as \"accessible here to every body, without distinction of age or color\",[54] but when he sponsored six Black men to join, only two other members of three hundred voted to accept them.[55] In May, Neal used his magazine to call out his fellow gymnasts' racial prejudice. He ended his involvement with the gym shortly thereafter.[56]\n",
            "\n",
            "Feminism\n",
            "Neal's writing on gender and women's rights in The Yankee show his focus moving beyond inter-gender social manners and female educational opportunities and toward women's economic and political rights.[57] In the first issue of the second volume, he asserted that unmarried women are treated unfairly \"as if it were better for a woman to marry anybody than not to marry at all; or even to marry one that was not her selected and preferred of all than to go unmarried to her grave.\"[58] The article \"Rights of Women\" (March 5, 1829) includes some of the \"angriest and most assertive feminist claims\"[59] of his career, saying of coverture and suffrage that:\n",
            "\n",
            "The truth is, that women are not citizens here; they pay taxes without being represented ...; if they are represented, it is by those whose interest, instead of being included in theirs, is directly opposed to theirs ...; they are not eligible to office; and they are not, nor is their property protected at law. So much for the equality of the sexes here ....[60]\n",
            "\n",
            "The solution, which he offered in \"Woman\" (March 26, 1828), was female solidarity and organizing to secure economic and political rights: \"If woman would act with woman, there would be a stop to our tyranny\". The Yankee also promoted female editors like Sarah Josepha Hale and Frances Harriet Whipple,[61] and proclaimed the example of economic freedom these women provided: \"We hope to see the day when she-editors will be as common as he-editors; and when our women of all ages ... will be able to maintain herself, without being obliged to marry for bread.\"[62]\n",
            "\n",
            "In other articles, The Yankee affirmed intellectual equality between men and women, opining that \"When minds meet, all distinctions of sex are abolished\"[63] and \"women are not inferior to men; they are unlike men. They cannot do all that men may do – any more than men may do all that women may do.\"[64]\n",
            "\n",
            "New England\n",
            "The magazine's title word, Yankee, is a demonym used to refer to people from Maine and the other New England states.[65] Holding his native state in high regard, Neal in the third issue of The Yankee claimed: \"Her magnitude, her resources, and her character, we believe, are neither appreciated nor understood by the chief men\" and the \"great mass of the American people.\"[66] To correct this, he published articles written by himself and others detailing the region's customs, traditions, and speech, particularly the series \"Live Yankees\" (March–June 1828), \"New England As It Was\" (March–November 1828), and \"New England As It Is\" (March–November 1828).[67] He juxtaposed articles by separate authors with conflicting views and inserted his own editorial footnotes into others' essays to encourage discourse over the region's identity.[68] Nineteenth-century American regionalists are known for sentimentally posing rural traditions in conflict with America's urbanization. In contrast, The Yankee presented the country's regions in a state of constant cultural evolution that beckons but thwarts characterization.[69]\n",
            "\n",
            "Feuds\n",
            "The first volume of The Yankee (January 1 – December 24, 1828) documents literary feuds between Neal and other New England journalists like William Lloyd Garrison, Francis Ormand Jonathan Smith, and Joseph T. Buckingham.[70] Tensions between Neal and Garrison started with Garrison's denunciation of Neal's literary criticism in Blackwood's Magazine (1824–1825) as a \"renegade's base attempt to assassinate the reputation of this country\"[71] and continued with Neal's claim in The Yankee that Garrison was fired from his editorial position for attacking Neal in the paper.[72] Journalist and historian Edward H. Elwell characterized Neal's willingness to publish these inflammatory back-and-forth letters and essays as the embodiment of \"impulsive honesty and fair play\".[73] Neal stopped after receiving complaints from subscribers, which he also published in the magazine.[74]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "text_preprocessed1 = preprocessing_text(deepcopy(text), language=\"english\")\n",
        "text_preprocessed2 = preprocessing_text(deepcopy(text), language=\"english\", remove_stopwords=True, remove_punctuation=True, remove_nums=True)\n",
        "text_preprocessed3 = preprocessing_text(deepcopy(text), language=\"english\", remove_stopwords=True, remove_punctuation=True, remove_nums=False, convert_nums=True)\n",
        "text_preprocessed4 = preprocessing_text(deepcopy(text), language=\"english\", convert_nums=True)\n",
        "text_preprocessed5 = preprocessing_text(deepcopy(text), language=\"english\", convert_nums=True, lemmatization=True)"
      ],
      "metadata": {
        "id": "Q9DOm2ZDU0H4"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter \n",
        "\n",
        "vocabulary1 = Counter(text_preprocessed1)\n",
        "vocabulary2 = Counter(text_preprocessed2)\n",
        "vocabulary3 = Counter(text_preprocessed3)\n",
        "vocabulary4 = Counter(text_preprocessed4)\n",
        "vocabulary5 = Counter(text_preprocessed5)"
      ],
      "metadata": {
        "id": "8QEMg3SBYHn4"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of tokens for the first type of preprocessing: ', len(vocabulary1))\n",
        "print('Number of tokens for the first type of preprocessing: ', len(vocabulary2))\n",
        "print('Number of tokens for the first type of preprocessing: ', len(vocabulary3))\n",
        "print('Number of tokens for the first type of preprocessing: ', len(vocabulary4))\n",
        "print('Number of tokens for the first type of preprocessing: ', len(vocabulary5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SryhnqePaYie",
        "outputId": "511690cb-e0ac-49d8-e47c-9228d75690eb"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens for the first type of preprocessing:  999\n",
            "Number of tokens for the first type of preprocessing:  888\n",
            "Number of tokens for the first type of preprocessing:  897\n",
            "Number of tokens for the first type of preprocessing:  999\n",
            "Number of tokens for the first type of preprocessing:  960\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "nltk_tokenization_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}